{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYzOYmYvpaiq",
        "outputId": "a85d93c5-ee20-487d-aea8-b48ef8e96162"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "g++ is already the newest version (4:11.2.0-1ubuntu1).\n",
            "swig is already the newest version (4.0.2-1ubuntu1).\n",
            "python3-dev is already the newest version (3.10.6-1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 43 not upgraded.\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.11/dist-packages (from gymnasium[box2d]) (4.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Collecting sympy==1.13.1 (from torch)\n",
            "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sympy\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.12\n",
            "    Uninstalling sympy-1.12:\n",
            "      Successfully uninstalled sympy-1.12\n",
            "Successfully installed sympy-1.13.1\n",
            "Collecting sympy==1.12\n",
            "  Using cached sympy-1.12-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.11/dist-packages (from sympy==1.12) (1.3.0)\n",
            "Using cached sympy-1.12-py3-none-any.whl (5.7 MB)\n",
            "Installing collected packages: sympy\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.13.1\n",
            "    Uninstalling sympy-1.13.1:\n",
            "      Successfully uninstalled sympy-1.13.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires sympy==1.13.1; python_version >= \"3.9\", but you have sympy 1.12 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed sympy-1.12\n"
          ]
        }
      ],
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y swig g++ python3-dev\n",
        "!pip install gymnasium[box2d] torch\n",
        "!pip install sympy==1.12"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Beta\n",
        "from torch.utils.data.sampler import BatchSampler, SubsetRandomSampler\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.cuda.amp import GradScaler, autocast\n",
        "import time\n",
        "import datetime\n",
        "from collections import deque\n",
        "import random\n",
        "import os\n",
        "\n",
        "ENV_NAME = \"BipedalWalker-v3\"\n",
        "RANDOM_SEED = 0\n",
        "LEARNING_RATE = 3e-4\n",
        "DISCOUNT = 0.99\n",
        "LAMBDA = 0.95\n",
        "CLIP_VALUE = 0.2\n",
        "TRAIN_EPOCHS = 10\n",
        "MINIBATCH = 64\n",
        "ROLLOUT_SIZE = 2048\n",
        "NEURONS = 256\n",
        "SHOW = False\n",
        "SAVE_INTERVAL = 100000\n",
        "MODEL_PATH = \"ppo_models\"\n",
        "DECAY_LR = True\n",
        "MIXED_PRECISION = True\n",
        "\n",
        "GOAL = 300\n",
        "AVG_WINDOW = 20\n",
        "MAX_EPISODES = 600\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(RANDOM_SEED)\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "    torch.backends.cudnn.deterministic = False\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "else:\n",
        "    MIXED_PRECISION = False\n",
        "\n",
        "os.makedirs(MODEL_PATH, exist_ok=True)\n",
        "\n",
        "compute_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {compute_device}\")\n",
        "if MIXED_PRECISION:\n",
        "    print(\"Using Mixed Precision Training\")\n",
        "if DECAY_LR:\n",
        "    print(\"Using Linear Learning Rate Decay\")\n",
        "\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.value_layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.value_layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.value_output = nn.Linear(hidden_size, 1)\n",
        "        self.policy_layer1 = nn.Linear(input_size, hidden_size)\n",
        "        self.policy_layer2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.alpha_head = nn.Linear(hidden_size, output_size)\n",
        "        self.beta_head = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        v = F.relu(self.value_layer1(x))\n",
        "        v = F.relu(self.value_layer2(v))\n",
        "        value = self.value_output(v)\n",
        "        p = F.relu(self.policy_layer1(x))\n",
        "        p = F.relu(self.policy_layer2(p))\n",
        "        alpha = F.softplus(self.alpha_head(p)) + 1.0\n",
        "        beta = F.softplus(self.beta_head(p)) + 1.0\n",
        "        return value, alpha, beta\n",
        "\n",
        "class Agent:\n",
        "    def __init__(self, obs_size, act_size, hidden_size, lr, gamma, lambda_, clip, epochs, batch_size, total_timesteps):\n",
        "        self.discount = gamma\n",
        "        self.lambda_param = lambda_\n",
        "        self.clip_param = clip\n",
        "        self.train_epochs = epochs\n",
        "        self.minibatch_size = batch_size\n",
        "        self.max_steps = total_timesteps\n",
        "\n",
        "        self.model = NeuralNet(obs_size, act_size, hidden_size).to(compute_device)\n",
        "        self.optim = optim.Adam(self.model.parameters(), lr=lr, eps=1e-5)\n",
        "\n",
        "        if DECAY_LR:\n",
        "            lr_func = lambda step: max(1.0 - float(step) / float(self.max_steps), 0)\n",
        "            self.lr_scheduler = LambdaLR(self.optim, lr_lambda=lr_func)\n",
        "        else:\n",
        "            self.lr_scheduler = None\n",
        "\n",
        "        self.grad_scaler = GradScaler(enabled=MIXED_PRECISION)\n",
        "        self.memory = {'obs': [], 'acts': [], 'rews': [], 'next_obs': [], 'logps': [], 'dones': []}\n",
        "        self.current_steps = 0\n",
        "\n",
        "    def remember(self, obs, act, rew, next_obs, logp, done):\n",
        "        self.memory['obs'].append(obs)\n",
        "        self.memory['acts'].append(act.cpu())\n",
        "        self.memory['rews'].append(rew)\n",
        "        self.memory['next_obs'].append(next_obs)\n",
        "        self.memory['logps'].append(logp.cpu())\n",
        "        self.memory['dones'].append(done)\n",
        "\n",
        "    def get_action(self, state, low, high):\n",
        "        state_tensor = torch.tensor(state, dtype=torch.float32, device=compute_device).unsqueeze(0)\n",
        "        with torch.no_grad():\n",
        "            with autocast(enabled=MIXED_PRECISION):\n",
        "                _, alpha, beta = self.model(state_tensor)\n",
        "        dist = Beta(alpha, beta)\n",
        "        action_norm = dist.sample()\n",
        "        log_prob = dist.log_prob(action_norm).sum(axis=-1)\n",
        "        action = low + (high - low) * action_norm.cpu().numpy()[0]\n",
        "        action = np.clip(action, low, high)\n",
        "        return action, action_norm.squeeze(0), log_prob.squeeze(0)\n",
        "\n",
        "    def learn(self):\n",
        "        if not self.memory['obs']:\n",
        "            return 0.0, 0.0, 0.0\n",
        "\n",
        "        obs_tensor = torch.tensor(np.array(self.memory['obs']), dtype=torch.float32, device=compute_device)\n",
        "        acts_tensor = torch.stack(self.memory['acts']).to(compute_device)\n",
        "        rews_tensor = torch.tensor(np.array(self.memory['rews']), dtype=torch.float32, device=compute_device).unsqueeze(1)\n",
        "        next_obs_tensor = torch.tensor(np.array(self.memory['next_obs']), dtype=torch.float32, device=compute_device)\n",
        "        old_logps_tensor = torch.stack(self.memory['logps']).to(compute_device)\n",
        "        masks_tensor = torch.tensor([1.0 - float(d) for d in self.memory['dones']], dtype=torch.float32, device=compute_device).unsqueeze(1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            with autocast(enabled=MIXED_PRECISION):\n",
        "                values, _, _ = self.model(obs_tensor)\n",
        "                next_values, _, _ = self.model(next_obs_tensor)\n",
        "            values = values.squeeze(-1)\n",
        "            next_values = next_values.squeeze(-1)\n",
        "            rews_tensor = rews_tensor.squeeze(-1)\n",
        "            masks_tensor = masks_tensor.squeeze(-1)\n",
        "\n",
        "            deltas = rews_tensor + self.discount * next_values * masks_tensor - values\n",
        "            advantages = torch.zeros_like(deltas)\n",
        "            last_gae = 0.0\n",
        "            for t in reversed(range(len(deltas))):\n",
        "                last_gae = deltas[t] + self.discount * self.lambda_param * masks_tensor[t] * last_gae\n",
        "                advantages[t] = last_gae\n",
        "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "            targets = advantages + values\n",
        "\n",
        "        avg_advantage = advantages.abs().mean().item()\n",
        "        policy_loss = 0.0\n",
        "        value_loss = 0.0\n",
        "        entropy_loss = 0.0\n",
        "\n",
        "        for _ in range(self.train_epochs):\n",
        "            batch_indices = BatchSampler(SubsetRandomSampler(range(len(obs_tensor))), self.minibatch_size, drop_last=False)\n",
        "            for idx in batch_indices:\n",
        "                obs_batch = obs_tensor[idx]\n",
        "                acts_batch = acts_tensor[idx]\n",
        "                old_logps_batch = old_logps_tensor[idx]\n",
        "                advantages_batch = advantages[idx]\n",
        "                targets_batch = targets[idx]\n",
        "\n",
        "                with autocast(enabled=MIXED_PRECISION):\n",
        "                    value_pred, alpha, beta = self.model(obs_batch)\n",
        "                    value_pred = value_pred.squeeze(-1)\n",
        "                    dist = Beta(alpha, beta)\n",
        "                    new_logps = dist.log_prob(acts_batch).sum(axis=-1)\n",
        "                    entropy = dist.entropy().sum(axis=-1).mean()\n",
        "                    ratios = torch.exp(new_logps - old_logps_batch)\n",
        "                    surr1 = ratios * advantages_batch\n",
        "                    surr2 = torch.clamp(ratios, 1.0 - self.clip_param, 1.0 + self.clip_param) * advantages_batch\n",
        "                    p_loss = -torch.min(surr1, surr2).mean()\n",
        "                    v_loss = F.mse_loss(value_pred, targets_batch)\n",
        "                    total_loss = p_loss + 0.5 * v_loss - 0.01 * entropy\n",
        "\n",
        "                self.optim.zero_grad()\n",
        "                self.grad_scaler.scale(total_loss).backward()\n",
        "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 0.5)\n",
        "                self.grad_scaler.step(self.optim)\n",
        "                self.grad_scaler.update()\n",
        "\n",
        "                policy_loss += p_loss.item()\n",
        "                value_loss += v_loss.item()\n",
        "                entropy_loss += entropy.item()\n",
        "\n",
        "        if self.lr_scheduler:\n",
        "            self.current_steps += len(obs_tensor)\n",
        "            temp_optim = optim.Adam(self.model.parameters(), lr=0)\n",
        "            temp_optim.load_state_dict(self.optim.state_dict())\n",
        "            temp_optim.state = self.optim.state\n",
        "            for group in temp_optim.param_groups:\n",
        "                group['initial_lr'] = LEARNING_RATE\n",
        "            self.lr_scheduler.optimizer = temp_optim\n",
        "            self.lr_scheduler.last_epoch = self.current_steps\n",
        "            self.lr_scheduler.step()\n",
        "            current_lr = self.lr_scheduler.get_last_lr()[0]\n",
        "            for group in self.optim.param_groups:\n",
        "                group['lr'] = current_lr\n",
        "            self.lr_scheduler.optimizer = self.optim\n",
        "\n",
        "        self.memory = {'obs': [], 'acts': [], 'rews': [], 'next_obs': [], 'logps': [], 'dones': []}\n",
        "        batches = len(obs_tensor) / self.minibatch_size\n",
        "        updates = self.train_epochs * batches\n",
        "        avg_p_loss = policy_loss / updates\n",
        "        avg_v_loss = value_loss / updates\n",
        "        avg_ent = entropy_loss / updates\n",
        "\n",
        "        return avg_p_loss, avg_v_loss, avg_ent, avg_advantage\n",
        "\n",
        "def test_agent(agent, env, low, high, test_episodes=5, render=False):\n",
        "    total_reward = 0.0\n",
        "    print(f\"\\nEvaluating ({test_episodes} episodes)\")\n",
        "    for i in range(test_episodes):\n",
        "        state, _ = env.reset()\n",
        "        episode_reward = 0.0\n",
        "        done = False\n",
        "        truncated = False\n",
        "        step_count = 0\n",
        "        max_steps = getattr(env.spec, 'max_episode_steps', 1600)\n",
        "\n",
        "        while not done and not truncated:\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32, device=compute_device).unsqueeze(0)\n",
        "            with torch.no_grad():\n",
        "                with autocast(enabled=MIXED_PRECISION):\n",
        "                    _, alpha, beta = agent.model(state_tensor)\n",
        "            action_norm = (alpha / (alpha + beta)).squeeze(0)\n",
        "            action = low + (high - low) * action_norm.cpu().numpy()\n",
        "            action = np.clip(action, low, high)\n",
        "\n",
        "            next_state, reward, done, truncated, _ = env.step(action)\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "            step_count += 1\n",
        "\n",
        "            if render:\n",
        "                try:\n",
        "                    env.render()\n",
        "                    time.sleep(0.01)\n",
        "                except:\n",
        "                    render = False\n",
        "\n",
        "            if step_count > max_steps + 100:\n",
        "                truncated = True\n",
        "\n",
        "        print(f\"Episode {i+1}/{test_episodes} | Reward: {episode_reward:.2f} | Steps: {step_count}\")\n",
        "        total_reward += episode_reward\n",
        "\n",
        "    average = total_reward / test_episodes\n",
        "    print(f\"Average Reward: {average:.2f}\")\n",
        "    return average\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    training_env = gym.make(ENV_NAME, render_mode=None)\n",
        "    testing_env = gym.make(ENV_NAME, render_mode=\"human\" if SHOW else None)\n",
        "\n",
        "    obs_size = training_env.observation_space.shape[0]\n",
        "    act_size = training_env.action_space.shape[0]\n",
        "    act_low = training_env.action_space.low\n",
        "    act_high = training_env.action_space.high\n",
        "\n",
        "    total_timesteps = MAX_EPISODES * 1000\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    print(f\"Environment: {ENV_NAME}\")\n",
        "    print(f\"State Dim: {obs_size}, Action Dim: {act_size}\")\n",
        "    print(f\"Action Range: {act_low} to {act_high}\")\n",
        "    print(f\"Update Interval: {ROLLOUT_SIZE}\")\n",
        "    print(f\"PPO Epochs: {TRAIN_EPOCHS}, Batch Size: {MINIBATCH}\")\n",
        "    print(f\"Device: {compute_device}\")\n",
        "    print(f\"AMP: {MIXED_PRECISION}\")\n",
        "    print(f\"LR Decay: {DECAY_LR} (Initial: {LEARNING_RATE}, Steps: {total_timesteps})\")\n",
        "    print(f\"Model Dir: {MODEL_PATH}\")\n",
        "    print(f\"Stopping Conditions:\")\n",
        "    print(f\"  Reward >= {GOAL} over {AVG_WINDOW} episodes\")\n",
        "    print(f\"  Max Episodes: {MAX_EPISODES}\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "    ppo_agent = Agent(obs_size, act_size, NEURONS, LEARNING_RATE, DISCOUNT, LAMBDA, CLIP_VALUE, TRAIN_EPOCHS, MINIBATCH, total_timesteps)\n",
        "\n",
        "    step_count = 0\n",
        "    episode_count = 0\n",
        "    state, _ = training_env.reset(seed=RANDOM_SEED)\n",
        "    episode_reward = 0.0\n",
        "    episode_steps = 0\n",
        "\n",
        "    start_time = time.time()\n",
        "    reward_history = deque(maxlen=AVG_WINDOW)\n",
        "    success = False\n",
        "\n",
        "    print(\"Training started...\")\n",
        "\n",
        "    while episode_count < MAX_EPISODES and not success:\n",
        "        collected_steps = 0\n",
        "        for _ in range(ROLLOUT_SIZE):\n",
        "            episode_steps += 1\n",
        "            step_count += 1\n",
        "            collected_steps += 1\n",
        "\n",
        "            env_action, norm_action, log_prob = ppo_agent.get_action(state, act_low, act_high)\n",
        "            next_state, reward, done, truncated, _ = training_env.step(env_action)\n",
        "            ppo_agent.remember(state, norm_action, reward, next_state, log_prob, done or truncated)\n",
        "\n",
        "            state = next_state\n",
        "            episode_reward += reward\n",
        "\n",
        "            if step_count % SAVE_INTERVAL == 0:\n",
        "                model_path = os.path.join(MODEL_PATH, f'ppo_{ENV_NAME}_checkpoint_{step_count}.pth')\n",
        "                try:\n",
        "                    torch.save(ppo_agent.model.state_dict(), model_path)\n",
        "                    print(f\"Checkpoint saved: {model_path} at step {step_count}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to save checkpoint: {e}\")\n",
        "\n",
        "            if done or truncated:\n",
        "                episode_count += 1\n",
        "                reward_history.append(episode_reward)\n",
        "\n",
        "                current_lr = ppo_agent.optim.param_groups[0]['lr']\n",
        "                log_msg = f\"Episode: {episode_count}/{MAX_EPISODES}, Steps: {step_count}, LR: {current_lr:.2e}, Reward: {episode_reward:.2f}\"\n",
        "\n",
        "                if len(reward_history) == AVG_WINDOW:\n",
        "                    avg_reward = np.mean(reward_history)\n",
        "                    log_msg += f\", Avg({AVG_WINDOW}): {avg_reward:.2f}\"\n",
        "                    if avg_reward >= GOAL:\n",
        "                        success = True\n",
        "                        print(log_msg)\n",
        "                        print(f\"Solved! Average reward {avg_reward:.2f} >= {GOAL}\")\n",
        "                        break\n",
        "                else:\n",
        "                    log_msg += f\", Avg({len(reward_history)}/{AVG_WINDOW}): ---\"\n",
        "\n",
        "                print(log_msg)\n",
        "\n",
        "                state, _ = training_env.reset()\n",
        "                episode_reward = 0.0\n",
        "                episode_steps = 0\n",
        "\n",
        "                if success or episode_count >= MAX_EPISODES:\n",
        "                    break\n",
        "\n",
        "        if len(ppo_agent.memory['obs']) > 0:\n",
        "            p_loss, v_loss, ent, adv = ppo_agent.learn()\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    time_str = str(datetime.timedelta(seconds=int(training_time)))\n",
        "    print(f\"\\nTraining completed\")\n",
        "    if success:\n",
        "        print(f\"Reason: Reward threshold reached\")\n",
        "    elif episode_count >= MAX_EPISODES:\n",
        "        print(f\"Reason: Max episodes reached\")\n",
        "    else:\n",
        "        print(f\"Reason: Unknown\")\n",
        "\n",
        "    print(f\"Total Episodes: {episode_count}\")\n",
        "    print(f\"Total Steps: {step_count}\")\n",
        "    print(f\"Time: {time_str}\")\n",
        "\n",
        "    final_model_path = os.path.join(MODEL_PATH, f'ppo_{ENV_NAME}_final_ep{episode_count}_step{step_count}.pth')\n",
        "    try:\n",
        "        torch.save(ppo_agent.model.state_dict(), final_model_path)\n",
        "        print(f\"Final model saved: {final_model_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to save final model: {e}\")\n",
        "\n",
        "    print(\"\\nFinal evaluation\")\n",
        "    final_score = test_agent(ppo_agent, testing_env, act_low, act_high, 10, SHOW)\n",
        "    print(f\"Final evaluation score: {final_score:.2f}\")\n",
        "\n",
        "    training_env.close()\n",
        "    testing_env.close()\n",
        "    print(\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRcVn5W3qI2-",
        "outputId": "23870ac9-80b0-4d87-cd52-d31d129b2b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Using Linear Learning Rate Decay\n",
            "------------------------------\n",
            "Environment: BipedalWalker-v3\n",
            "State Dim: 24, Action Dim: 4\n",
            "Action Low: [-1. -1. -1. -1.], Action High: [1. 1. 1. 1.]\n",
            "Update Interval: 2048\n",
            "PPO Epochs: 10, Batch Size: 64\n",
            "Using Device: cpu\n",
            "Mixed Precision: False\n",
            "LR Decay: True (Initial: 0.0003, Est. Total Steps: 600000)\n",
            "Saving models to: ppo_models\n",
            "Stopping Conditions:\n",
            "  1. Avg Reward >= 300 over last 20 episodes.\n",
            "  2. Max Episodes Reached: 600\n",
            "------------------------------\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-e89e822356da>:121: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = GradScaler(enabled=use_mixed_precision)\n",
            "<ipython-input-3-e89e822356da>:144: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_mixed_precision):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep: 1/600, Timesteps: 1600, LR: 3.00e-04, Ep Reward: -66.83, Avg(1/20): ---\n",
            "Ep: 2/600, Timesteps: 1657, LR: 3.00e-04, Ep Reward: -99.37, Avg(2/20): ---\n",
            "Ep: 3/600, Timesteps: 1717, LR: 3.00e-04, Ep Reward: -98.16, Avg(3/20): ---\n",
            "Ep: 4/600, Timesteps: 1783, LR: 3.00e-04, Ep Reward: -98.20, Avg(4/20): ---\n",
            "Ep: 5/600, Timesteps: 1851, LR: 3.00e-04, Ep Reward: -113.23, Avg(5/20): ---\n",
            "Ep: 6/600, Timesteps: 1973, LR: 3.00e-04, Ep Reward: -119.21, Avg(6/20): ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-e89e822356da>:177: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_mixed_precision):\n",
            "<ipython-input-3-e89e822356da>:227: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_mixed_precision):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep: 7/600, Timesteps: 2058, LR: 2.99e-04, Ep Reward: -104.66, Avg(7/20): ---\n",
            "Ep: 8/600, Timesteps: 2180, LR: 2.99e-04, Ep Reward: -117.45, Avg(8/20): ---\n",
            "Ep: 9/600, Timesteps: 2243, LR: 2.99e-04, Ep Reward: -111.46, Avg(9/20): ---\n",
            "Ep: 10/600, Timesteps: 3843, LR: 2.99e-04, Ep Reward: -57.68, Avg(10/20): ---\n",
            "Ep: 11/600, Timesteps: 3905, LR: 2.99e-04, Ep Reward: -110.11, Avg(11/20): ---\n",
            "Ep: 12/600, Timesteps: 3954, LR: 2.99e-04, Ep Reward: -108.59, Avg(12/20): ---\n",
            "Ep: 13/600, Timesteps: 4093, LR: 2.99e-04, Ep Reward: -125.16, Avg(13/20): ---\n",
            "Ep: 14/600, Timesteps: 4158, LR: 2.98e-04, Ep Reward: -110.51, Avg(14/20): ---\n",
            "Ep: 15/600, Timesteps: 5758, LR: 2.98e-04, Ep Reward: -62.31, Avg(15/20): ---\n",
            "Ep: 16/600, Timesteps: 5971, LR: 2.98e-04, Ep Reward: -130.23, Avg(16/20): ---\n",
            "Ep: 17/600, Timesteps: 6021, LR: 2.98e-04, Ep Reward: -109.28, Avg(17/20): ---\n",
            "Ep: 18/600, Timesteps: 6110, LR: 2.98e-04, Ep Reward: -119.67, Avg(18/20): ---\n",
            "Ep: 19/600, Timesteps: 6182, LR: 2.97e-04, Ep Reward: -114.30, Avg(19/20): ---\n",
            "Ep: 20/600, Timesteps: 6300, LR: 2.97e-04, Ep Reward: -119.45, Avg(20): -104.79\n",
            "Ep: 21/600, Timesteps: 6399, LR: 2.97e-04, Ep Reward: -113.91, Avg(20): -107.15\n",
            "Ep: 22/600, Timesteps: 6480, LR: 2.97e-04, Ep Reward: -121.46, Avg(20): -108.25\n",
            "Ep: 23/600, Timesteps: 6547, LR: 2.97e-04, Ep Reward: -99.26, Avg(20): -108.31\n",
            "Ep: 24/600, Timesteps: 8147, LR: 2.97e-04, Ep Reward: -69.65, Avg(20): -106.88\n",
            "Ep: 25/600, Timesteps: 9747, LR: 2.96e-04, Ep Reward: -73.42, Avg(20): -104.89\n",
            "Ep: 26/600, Timesteps: 9803, LR: 2.96e-04, Ep Reward: -112.19, Avg(20): -104.54\n",
            "Ep: 27/600, Timesteps: 9852, LR: 2.96e-04, Ep Reward: -114.29, Avg(20): -105.02\n",
            "Ep: 28/600, Timesteps: 11452, LR: 2.95e-04, Ep Reward: -68.16, Avg(20): -102.55\n",
            "Ep: 29/600, Timesteps: 11543, LR: 2.95e-04, Ep Reward: -110.54, Avg(20): -102.51\n",
            "Ep: 30/600, Timesteps: 13143, LR: 2.94e-04, Ep Reward: -59.33, Avg(20): -102.59\n",
            "Ep: 31/600, Timesteps: 14743, LR: 2.93e-04, Ep Reward: -78.73, Avg(20): -101.02\n",
            "Ep: 32/600, Timesteps: 14812, LR: 2.93e-04, Ep Reward: -113.39, Avg(20): -101.26\n",
            "Ep: 33/600, Timesteps: 14923, LR: 2.93e-04, Ep Reward: -117.88, Avg(20): -100.90\n",
            "Ep: 34/600, Timesteps: 16127, LR: 2.93e-04, Ep Reward: -173.20, Avg(20): -104.03\n",
            "Ep: 35/600, Timesteps: 17727, LR: 2.92e-04, Ep Reward: -57.42, Avg(20): -103.79\n",
            "Ep: 36/600, Timesteps: 17843, LR: 2.92e-04, Ep Reward: -120.79, Avg(20): -103.32\n",
            "Ep: 37/600, Timesteps: 19443, LR: 2.91e-04, Ep Reward: -59.28, Avg(20): -100.82\n",
            "Ep: 38/600, Timesteps: 21043, LR: 2.90e-04, Ep Reward: -49.29, Avg(20): -97.30\n",
            "Ep: 39/600, Timesteps: 22643, LR: 2.89e-04, Ep Reward: -77.34, Avg(20): -95.45\n",
            "Ep: 40/600, Timesteps: 22703, LR: 2.89e-04, Ep Reward: -110.28, Avg(20): -94.99\n",
            "Ep: 41/600, Timesteps: 22763, LR: 2.89e-04, Ep Reward: -103.98, Avg(20): -94.49\n",
            "Ep: 42/600, Timesteps: 22891, LR: 2.89e-04, Ep Reward: -120.81, Avg(20): -94.46\n",
            "Ep: 43/600, Timesteps: 22973, LR: 2.89e-04, Ep Reward: -116.24, Avg(20): -95.31\n",
            "Ep: 44/600, Timesteps: 24573, LR: 2.89e-04, Ep Reward: -55.33, Avg(20): -94.59\n",
            "Ep: 45/600, Timesteps: 24678, LR: 2.88e-04, Ep Reward: -116.22, Avg(20): -96.73\n",
            "Ep: 46/600, Timesteps: 24737, LR: 2.88e-04, Ep Reward: -117.56, Avg(20): -97.00\n",
            "Ep: 47/600, Timesteps: 26337, LR: 2.88e-04, Ep Reward: -65.58, Avg(20): -94.57\n",
            "Ep: 48/600, Timesteps: 26425, LR: 2.88e-04, Ep Reward: -99.23, Avg(20): -96.12\n",
            "Ep: 49/600, Timesteps: 26495, LR: 2.88e-04, Ep Reward: -121.25, Avg(20): -96.66\n",
            "Ep: 50/600, Timesteps: 26570, LR: 2.88e-04, Ep Reward: -98.63, Avg(20): -98.62\n",
            "Ep: 51/600, Timesteps: 26635, LR: 2.87e-04, Ep Reward: -101.41, Avg(20): -99.76\n",
            "Ep: 52/600, Timesteps: 26712, LR: 2.87e-04, Ep Reward: -101.95, Avg(20): -99.18\n",
            "Ep: 53/600, Timesteps: 26805, LR: 2.87e-04, Ep Reward: -108.28, Avg(20): -98.70\n",
            "Ep: 54/600, Timesteps: 26887, LR: 2.87e-04, Ep Reward: -98.92, Avg(20): -94.99\n",
            "Ep: 55/600, Timesteps: 28487, LR: 2.87e-04, Ep Reward: -50.24, Avg(20): -94.63\n",
            "Ep: 56/600, Timesteps: 28534, LR: 2.87e-04, Ep Reward: -105.75, Avg(20): -93.88\n",
            "Ep: 57/600, Timesteps: 28625, LR: 2.87e-04, Ep Reward: -122.63, Avg(20): -97.05\n",
            "Ep: 58/600, Timesteps: 28705, LR: 2.86e-04, Ep Reward: -99.71, Avg(20): -99.57\n",
            "Ep: 59/600, Timesteps: 30305, LR: 2.86e-04, Ep Reward: -61.34, Avg(20): -98.77\n",
            "Ep: 60/600, Timesteps: 30421, LR: 2.86e-04, Ep Reward: -99.52, Avg(20): -98.23\n",
            "Ep: 61/600, Timesteps: 30486, LR: 2.86e-04, Ep Reward: -99.09, Avg(20): -97.99\n",
            "Ep: 62/600, Timesteps: 30545, LR: 2.86e-04, Ep Reward: -105.74, Avg(20): -97.23\n",
            "Ep: 63/600, Timesteps: 30631, LR: 2.86e-04, Ep Reward: -117.15, Avg(20): -97.28\n",
            "Ep: 64/600, Timesteps: 30698, LR: 2.86e-04, Ep Reward: -109.75, Avg(20): -100.00\n",
            "Ep: 65/600, Timesteps: 30781, LR: 2.85e-04, Ep Reward: -99.33, Avg(20): -99.15\n",
            "Ep: 66/600, Timesteps: 32381, LR: 2.85e-04, Ep Reward: -79.86, Avg(20): -97.27\n",
            "Ep: 67/600, Timesteps: 33981, LR: 2.84e-04, Ep Reward: -82.93, Avg(20): -98.14\n",
            "Ep: 68/600, Timesteps: 35581, LR: 2.83e-04, Ep Reward: -57.02, Avg(20): -96.03\n",
            "Ep: 69/600, Timesteps: 37181, LR: 2.82e-04, Ep Reward: -57.00, Avg(20): -92.81\n",
            "Ep: 70/600, Timesteps: 37230, LR: 2.82e-04, Ep Reward: -114.41, Avg(20): -93.60\n",
            "Ep: 71/600, Timesteps: 37307, LR: 2.82e-04, Ep Reward: -100.00, Avg(20): -93.53\n",
            "Ep: 72/600, Timesteps: 37375, LR: 2.82e-04, Ep Reward: -101.26, Avg(20): -93.50\n",
            "Ep: 73/600, Timesteps: 38975, LR: 2.81e-04, Ep Reward: -49.81, Avg(20): -90.57\n",
            "Ep: 74/600, Timesteps: 39042, LR: 2.81e-04, Ep Reward: -111.29, Avg(20): -91.19\n",
            "Ep: 75/600, Timesteps: 40642, LR: 2.81e-04, Ep Reward: -90.23, Avg(20): -93.19\n",
            "Ep: 76/600, Timesteps: 42242, LR: 2.80e-04, Ep Reward: -50.94, Avg(20): -90.45\n",
            "Ep: 77/600, Timesteps: 42323, LR: 2.80e-04, Ep Reward: -101.63, Avg(20): -89.40\n",
            "Ep: 78/600, Timesteps: 43923, LR: 2.78e-04, Ep Reward: -54.16, Avg(20): -87.12\n",
            "Ep: 79/600, Timesteps: 43984, LR: 2.78e-04, Ep Reward: -101.08, Avg(20): -89.11\n",
            "Ep: 80/600, Timesteps: 44067, LR: 2.78e-04, Ep Reward: -96.69, Avg(20): -88.97\n",
            "Ep: 81/600, Timesteps: 44129, LR: 2.78e-04, Ep Reward: -115.46, Avg(20): -89.79\n",
            "Ep: 82/600, Timesteps: 44210, LR: 2.78e-04, Ep Reward: -100.10, Avg(20): -89.50\n",
            "Ep: 83/600, Timesteps: 45810, LR: 2.77e-04, Ep Reward: -76.01, Avg(20): -87.45\n",
            "Ep: 84/600, Timesteps: 45872, LR: 2.77e-04, Ep Reward: -112.13, Avg(20): -87.57\n",
            "Ep: 85/600, Timesteps: 45970, LR: 2.77e-04, Ep Reward: -119.26, Avg(20): -88.56\n",
            "Ep: 86/600, Timesteps: 47570, LR: 2.76e-04, Ep Reward: -82.60, Avg(20): -88.70\n",
            "Ep: 87/600, Timesteps: 49170, LR: 2.75e-04, Ep Reward: -48.98, Avg(20): -87.00\n",
            "Ep: 88/600, Timesteps: 50770, LR: 2.75e-04, Ep Reward: -72.60, Avg(20): -87.78\n",
            "Ep: 89/600, Timesteps: 50837, LR: 2.75e-04, Ep Reward: -113.93, Avg(20): -90.63\n",
            "Ep: 90/600, Timesteps: 52437, LR: 2.74e-04, Ep Reward: -50.62, Avg(20): -87.44\n",
            "Ep: 91/600, Timesteps: 52542, LR: 2.74e-04, Ep Reward: -116.20, Avg(20): -88.25\n",
            "Ep: 92/600, Timesteps: 54142, LR: 2.73e-04, Ep Reward: -71.45, Avg(20): -86.76\n",
            "Ep: 93/600, Timesteps: 55742, LR: 2.72e-04, Ep Reward: -66.77, Avg(20): -87.61\n",
            "Ep: 94/600, Timesteps: 55811, LR: 2.72e-04, Ep Reward: -104.08, Avg(20): -87.25\n",
            "Ep: 95/600, Timesteps: 57411, LR: 2.71e-04, Ep Reward: -70.54, Avg(20): -86.26\n",
            "Ep: 96/600, Timesteps: 57454, LR: 2.71e-04, Ep Reward: -109.48, Avg(20): -89.19\n",
            "Ep: 97/600, Timesteps: 57544, LR: 2.71e-04, Ep Reward: -102.94, Avg(20): -89.25\n",
            "Ep: 98/600, Timesteps: 57624, LR: 2.71e-04, Ep Reward: -116.27, Avg(20): -92.36\n",
            "Ep: 99/600, Timesteps: 59224, LR: 2.71e-04, Ep Reward: -56.08, Avg(20): -90.11\n",
            "Ep: 100/600, Timesteps: 59266, LR: 2.71e-04, Ep Reward: -107.82, Avg(20): -90.66\n",
            "Ep: 101/600, Timesteps: 59364, LR: 2.71e-04, Ep Reward: -122.83, Avg(20): -91.03\n",
            "Ep: 102/600, Timesteps: 60964, LR: 2.70e-04, Ep Reward: -54.43, Avg(20): -88.75\n",
            "Ep: 103/600, Timesteps: 62564, LR: 2.69e-04, Ep Reward: -67.54, Avg(20): -88.33\n",
            "Ep: 104/600, Timesteps: 62636, LR: 2.69e-04, Ep Reward: -123.25, Avg(20): -88.88\n",
            "Ep: 105/600, Timesteps: 62725, LR: 2.69e-04, Ep Reward: -113.96, Avg(20): -88.62\n",
            "Ep: 106/600, Timesteps: 62800, LR: 2.69e-04, Ep Reward: -99.64, Avg(20): -89.47\n",
            "Ep: 107/600, Timesteps: 64400, LR: 2.68e-04, Ep Reward: -74.86, Avg(20): -90.76\n",
            "Ep: 108/600, Timesteps: 64456, LR: 2.68e-04, Ep Reward: -114.15, Avg(20): -92.84\n",
            "Ep: 109/600, Timesteps: 64505, LR: 2.68e-04, Ep Reward: -109.66, Avg(20): -92.63\n",
            "Ep: 110/600, Timesteps: 64602, LR: 2.68e-04, Ep Reward: -118.83, Avg(20): -96.04\n",
            "Ep: 111/600, Timesteps: 64670, LR: 2.68e-04, Ep Reward: -116.51, Avg(20): -96.05\n",
            "Ep: 112/600, Timesteps: 64770, LR: 2.68e-04, Ep Reward: -118.82, Avg(20): -98.42\n",
            "Ep: 113/600, Timesteps: 64812, LR: 2.68e-04, Ep Reward: -109.50, Avg(20): -100.56\n",
            "Ep: 114/600, Timesteps: 64865, LR: 2.68e-04, Ep Reward: -114.65, Avg(20): -101.09\n",
            "Ep: 115/600, Timesteps: 66465, LR: 2.67e-04, Ep Reward: -60.31, Avg(20): -100.58\n",
            "Ep: 116/600, Timesteps: 66533, LR: 2.67e-04, Ep Reward: -108.98, Avg(20): -100.55\n",
            "Ep: 117/600, Timesteps: 66623, LR: 2.67e-04, Ep Reward: -108.65, Avg(20): -100.84\n",
            "Ep: 118/600, Timesteps: 66691, LR: 2.67e-04, Ep Reward: -112.68, Avg(20): -100.66\n",
            "Ep: 119/600, Timesteps: 68291, LR: 2.66e-04, Ep Reward: -47.60, Avg(20): -100.23\n",
            "Ep: 120/600, Timesteps: 68375, LR: 2.66e-04, Ep Reward: -112.12, Avg(20): -100.45\n",
            "Ep: 121/600, Timesteps: 68440, LR: 2.66e-04, Ep Reward: -115.69, Avg(20): -100.09\n",
            "Ep: 122/600, Timesteps: 68487, LR: 2.66e-04, Ep Reward: -111.91, Avg(20): -102.96\n",
            "Ep: 123/600, Timesteps: 70087, LR: 2.65e-04, Ep Reward: -62.64, Avg(20): -102.72\n",
            "Ep: 124/600, Timesteps: 71687, LR: 2.64e-04, Ep Reward: -55.86, Avg(20): -99.35\n",
            "Ep: 125/600, Timesteps: 73287, LR: 2.64e-04, Ep Reward: -48.20, Avg(20): -96.06\n",
            "Ep: 126/600, Timesteps: 73350, LR: 2.64e-04, Ep Reward: -101.32, Avg(20): -96.15\n",
            "Ep: 127/600, Timesteps: 74950, LR: 2.63e-04, Ep Reward: -44.91, Avg(20): -94.65\n",
            "Ep: 128/600, Timesteps: 76550, LR: 2.62e-04, Ep Reward: -45.77, Avg(20): -91.23\n",
            "Ep: 129/600, Timesteps: 78150, LR: 2.61e-04, Ep Reward: -46.55, Avg(20): -88.08\n",
            "Ep: 130/600, Timesteps: 79750, LR: 2.61e-04, Ep Reward: -46.59, Avg(20): -84.46\n",
            "Ep: 131/600, Timesteps: 81350, LR: 2.60e-04, Ep Reward: -61.70, Avg(20): -81.72\n",
            "Ep: 132/600, Timesteps: 81426, LR: 2.60e-04, Ep Reward: -105.84, Avg(20): -81.07\n",
            "Ep: 133/600, Timesteps: 81474, LR: 2.60e-04, Ep Reward: -107.97, Avg(20): -81.00\n",
            "Ep: 134/600, Timesteps: 81524, LR: 2.60e-04, Ep Reward: -108.88, Avg(20): -80.71\n",
            "Ep: 135/600, Timesteps: 81611, LR: 2.60e-04, Ep Reward: -110.29, Avg(20): -83.21\n",
            "Ep: 136/600, Timesteps: 83211, LR: 2.59e-04, Ep Reward: -53.69, Avg(20): -80.44\n",
            "Ep: 137/600, Timesteps: 83246, LR: 2.59e-04, Ep Reward: -106.42, Avg(20): -80.33\n",
            "Ep: 138/600, Timesteps: 84846, LR: 2.58e-04, Ep Reward: -45.39, Avg(20): -76.97\n",
            "Ep: 139/600, Timesteps: 84949, LR: 2.58e-04, Ep Reward: -113.47, Avg(20): -80.26\n",
            "Ep: 140/600, Timesteps: 85010, LR: 2.58e-04, Ep Reward: -112.20, Avg(20): -80.26\n",
            "Ep: 141/600, Timesteps: 86610, LR: 2.57e-04, Ep Reward: -68.14, Avg(20): -77.89\n",
            "Ep: 142/600, Timesteps: 88210, LR: 2.56e-04, Ep Reward: -35.96, Avg(20): -74.09\n",
            "Ep: 143/600, Timesteps: 88282, LR: 2.56e-04, Ep Reward: -108.37, Avg(20): -76.38\n",
            "Ep: 144/600, Timesteps: 88352, LR: 2.56e-04, Ep Reward: -117.67, Avg(20): -79.47\n",
            "Ep: 145/600, Timesteps: 89952, LR: 2.56e-04, Ep Reward: -66.05, Avg(20): -80.36\n",
            "Ep: 146/600, Timesteps: 91552, LR: 2.55e-04, Ep Reward: -37.56, Avg(20): -77.17\n",
            "Ep: 147/600, Timesteps: 93152, LR: 2.54e-04, Ep Reward: -63.91, Avg(20): -78.12\n",
            "Ep: 148/600, Timesteps: 93231, LR: 2.54e-04, Ep Reward: -122.63, Avg(20): -81.96\n",
            "Ep: 149/600, Timesteps: 93296, LR: 2.54e-04, Ep Reward: -115.60, Avg(20): -85.42\n",
            "Ep: 150/600, Timesteps: 93355, LR: 2.54e-04, Ep Reward: -107.68, Avg(20): -88.47\n",
            "Ep: 151/600, Timesteps: 93418, LR: 2.54e-04, Ep Reward: -101.63, Avg(20): -90.47\n",
            "Ep: 152/600, Timesteps: 95018, LR: 2.53e-04, Ep Reward: -34.83, Avg(20): -86.92\n",
            "Ep: 153/600, Timesteps: 95065, LR: 2.53e-04, Ep Reward: -107.69, Avg(20): -86.90\n",
            "Ep: 154/600, Timesteps: 96665, LR: 2.52e-04, Ep Reward: -41.20, Avg(20): -83.52\n",
            "Ep: 155/600, Timesteps: 98265, LR: 2.52e-04, Ep Reward: -59.09, Avg(20): -80.96\n",
            "Ep: 156/600, Timesteps: 99865, LR: 2.51e-04, Ep Reward: -54.21, Avg(20): -80.99\n",
            "\n",
            "--- Checkpoint Model Saved: ppo_models/ppo_ac_BipedalWalker-v3_ckpt_100000.pth at Timestep 100000 ---\n",
            "Ep: 157/600, Timesteps: 101465, LR: 2.50e-04, Ep Reward: -62.97, Avg(20): -78.81\n",
            "Ep: 158/600, Timesteps: 103065, LR: 2.49e-04, Ep Reward: -57.37, Avg(20): -79.41\n",
            "Ep: 159/600, Timesteps: 103109, LR: 2.49e-04, Ep Reward: -108.62, Avg(20): -79.17\n",
            "Ep: 160/600, Timesteps: 104709, LR: 2.48e-04, Ep Reward: -38.91, Avg(20): -75.50\n",
            "Ep: 161/600, Timesteps: 104779, LR: 2.48e-04, Ep Reward: -105.78, Avg(20): -77.39\n",
            "Ep: 162/600, Timesteps: 106379, LR: 2.48e-04, Ep Reward: -62.94, Avg(20): -78.74\n",
            "Ep: 163/600, Timesteps: 106451, LR: 2.48e-04, Ep Reward: -105.01, Avg(20): -78.57\n",
            "Ep: 164/600, Timesteps: 108051, LR: 2.47e-04, Ep Reward: -42.58, Avg(20): -74.81\n",
            "Ep: 165/600, Timesteps: 109651, LR: 2.46e-04, Ep Reward: -45.78, Avg(20): -73.80\n",
            "Ep: 166/600, Timesteps: 109868, LR: 2.46e-04, Ep Reward: -128.41, Avg(20): -78.34\n",
            "Ep: 167/600, Timesteps: 109939, LR: 2.46e-04, Ep Reward: -100.96, Avg(20): -80.19\n",
            "Ep: 168/600, Timesteps: 111539, LR: 2.45e-04, Ep Reward: -41.26, Avg(20): -76.13\n",
            "Ep: 169/600, Timesteps: 111581, LR: 2.45e-04, Ep Reward: -106.68, Avg(20): -75.68\n",
            "Ep: 170/600, Timesteps: 113181, LR: 2.44e-04, Ep Reward: -59.55, Avg(20): -73.27\n",
            "Ep: 171/600, Timesteps: 113233, LR: 2.44e-04, Ep Reward: -112.15, Avg(20): -73.80\n",
            "Ep: 172/600, Timesteps: 114833, LR: 2.43e-04, Ep Reward: -50.13, Avg(20): -74.56\n",
            "Ep: 173/600, Timesteps: 114930, LR: 2.43e-04, Ep Reward: -118.78, Avg(20): -75.12\n",
            "Ep: 174/600, Timesteps: 114985, LR: 2.43e-04, Ep Reward: -113.10, Avg(20): -78.71\n",
            "Ep: 175/600, Timesteps: 115042, LR: 2.43e-04, Ep Reward: -103.30, Avg(20): -80.93\n",
            "Ep: 176/600, Timesteps: 116642, LR: 2.43e-04, Ep Reward: -53.45, Avg(20): -80.89\n",
            "Ep: 177/600, Timesteps: 116730, LR: 2.43e-04, Ep Reward: -118.44, Avg(20): -83.66\n",
            "Ep: 178/600, Timesteps: 118330, LR: 2.42e-04, Ep Reward: -37.07, Avg(20): -82.65\n",
            "Ep: 179/600, Timesteps: 119930, LR: 2.41e-04, Ep Reward: -48.69, Avg(20): -79.65\n",
            "Ep: 180/600, Timesteps: 121530, LR: 2.40e-04, Ep Reward: -32.97, Avg(20): -79.35\n",
            "Ep: 181/600, Timesteps: 123130, LR: 2.39e-04, Ep Reward: -36.93, Avg(20): -75.91\n",
            "Ep: 182/600, Timesteps: 124730, LR: 2.39e-04, Ep Reward: -52.68, Avg(20): -75.40\n",
            "Ep: 183/600, Timesteps: 124781, LR: 2.39e-04, Ep Reward: -111.26, Avg(20): -75.71\n",
            "Ep: 184/600, Timesteps: 124864, LR: 2.39e-04, Ep Reward: -112.99, Avg(20): -79.23\n",
            "Ep: 185/600, Timesteps: 126464, LR: 2.38e-04, Ep Reward: -56.70, Avg(20): -79.78\n",
            "Ep: 186/600, Timesteps: 126546, LR: 2.38e-04, Ep Reward: -99.42, Avg(20): -78.33\n",
            "Ep: 187/600, Timesteps: 126615, LR: 2.38e-04, Ep Reward: -118.93, Avg(20): -79.23\n",
            "Ep: 188/600, Timesteps: 128215, LR: 2.37e-04, Ep Reward: -38.37, Avg(20): -79.08\n",
            "Ep: 189/600, Timesteps: 129815, LR: 2.35e-04, Ep Reward: -51.04, Avg(20): -76.30\n",
            "Ep: 190/600, Timesteps: 131415, LR: 2.34e-04, Ep Reward: -34.31, Avg(20): -75.04\n",
            "Ep: 191/600, Timesteps: 131482, LR: 2.34e-04, Ep Reward: -101.99, Avg(20): -74.53\n",
            "Ep: 192/600, Timesteps: 133082, LR: 2.34e-04, Ep Reward: -50.88, Avg(20): -74.57\n",
            "Ep: 193/600, Timesteps: 133149, LR: 2.33e-04, Ep Reward: -101.79, Avg(20): -73.72\n",
            "Ep: 194/600, Timesteps: 134749, LR: 2.33e-04, Ep Reward: -53.25, Avg(20): -70.72\n",
            "Ep: 195/600, Timesteps: 134805, LR: 2.33e-04, Ep Reward: -112.54, Avg(20): -71.19\n",
            "Ep: 196/600, Timesteps: 134866, LR: 2.33e-04, Ep Reward: -100.89, Avg(20): -73.56\n",
            "Ep: 197/600, Timesteps: 136466, LR: 2.32e-04, Ep Reward: -31.13, Avg(20): -69.19\n",
            "Ep: 198/600, Timesteps: 138066, LR: 2.31e-04, Ep Reward: -35.07, Avg(20): -69.09\n",
            "Ep: 199/600, Timesteps: 139666, LR: 2.30e-04, Ep Reward: -28.58, Avg(20): -68.09\n",
            "Ep: 200/600, Timesteps: 141266, LR: 2.30e-04, Ep Reward: -28.58, Avg(20): -67.87\n",
            "Ep: 201/600, Timesteps: 141350, LR: 2.29e-04, Ep Reward: -103.59, Avg(20): -71.20\n",
            "Ep: 202/600, Timesteps: 141470, LR: 2.29e-04, Ep Reward: -124.13, Avg(20): -74.77\n",
            "Ep: 203/600, Timesteps: 141512, LR: 2.29e-04, Ep Reward: -111.38, Avg(20): -74.78\n",
            "Ep: 204/600, Timesteps: 141581, LR: 2.29e-04, Ep Reward: -99.70, Avg(20): -74.11\n",
            "Ep: 205/600, Timesteps: 143181, LR: 2.29e-04, Ep Reward: -48.65, Avg(20): -73.71\n",
            "Ep: 206/600, Timesteps: 144781, LR: 2.28e-04, Ep Reward: -30.73, Avg(20): -70.28\n",
            "Ep: 207/600, Timesteps: 144874, LR: 2.28e-04, Ep Reward: -99.98, Avg(20): -69.33\n",
            "Ep: 208/600, Timesteps: 146474, LR: 2.27e-04, Ep Reward: -49.57, Avg(20): -69.89\n",
            "Ep: 209/600, Timesteps: 148074, LR: 2.26e-04, Ep Reward: -32.87, Avg(20): -68.98\n",
            "Ep: 210/600, Timesteps: 149674, LR: 2.25e-04, Ep Reward: -27.24, Avg(20): -68.63\n",
            "Ep: 211/600, Timesteps: 151274, LR: 2.25e-04, Ep Reward: -32.17, Avg(20): -65.13\n",
            "Ep: 212/600, Timesteps: 152874, LR: 2.24e-04, Ep Reward: -29.35, Avg(20): -64.06\n",
            "Ep: 213/600, Timesteps: 154474, LR: 2.23e-04, Ep Reward: -28.85, Avg(20): -60.41\n",
            "Ep: 214/600, Timesteps: 156074, LR: 2.22e-04, Ep Reward: -32.20, Avg(20): -59.36\n",
            "Ep: 215/600, Timesteps: 156140, LR: 2.22e-04, Ep Reward: -109.82, Avg(20): -59.22\n",
            "Ep: 216/600, Timesteps: 157740, LR: 2.21e-04, Ep Reward: -30.56, Avg(20): -55.71\n",
            "Ep: 217/600, Timesteps: 157802, LR: 2.21e-04, Ep Reward: -112.15, Avg(20): -59.76\n",
            "Ep: 218/600, Timesteps: 159402, LR: 2.21e-04, Ep Reward: -28.96, Avg(20): -59.45\n",
            "Ep: 219/600, Timesteps: 161002, LR: 2.20e-04, Ep Reward: -29.91, Avg(20): -59.52\n",
            "Ep: 220/600, Timesteps: 162602, LR: 2.19e-04, Ep Reward: -28.26, Avg(20): -59.50\n",
            "Ep: 221/600, Timesteps: 164202, LR: 2.18e-04, Ep Reward: -29.88, Avg(20): -55.82\n",
            "Ep: 222/600, Timesteps: 165802, LR: 2.18e-04, Ep Reward: -24.63, Avg(20): -50.84\n",
            "Ep: 223/600, Timesteps: 167402, LR: 2.17e-04, Ep Reward: -23.53, Avg(20): -46.45\n",
            "Ep: 224/600, Timesteps: 169002, LR: 2.16e-04, Ep Reward: -38.46, Avg(20): -43.39\n",
            "Ep: 225/600, Timesteps: 170602, LR: 2.15e-04, Ep Reward: -29.56, Avg(20): -42.43\n",
            "Ep: 226/600, Timesteps: 172202, LR: 2.14e-04, Ep Reward: -25.66, Avg(20): -42.18\n",
            "Ep: 227/600, Timesteps: 173802, LR: 2.14e-04, Ep Reward: -24.38, Avg(20): -38.40\n",
            "Ep: 228/600, Timesteps: 175402, LR: 2.13e-04, Ep Reward: -22.69, Avg(20): -37.06\n",
            "Ep: 229/600, Timesteps: 177002, LR: 2.12e-04, Ep Reward: -26.61, Avg(20): -36.74\n",
            "Ep: 230/600, Timesteps: 178602, LR: 2.11e-04, Ep Reward: -21.42, Avg(20): -36.45\n",
            "Ep: 231/600, Timesteps: 180202, LR: 2.11e-04, Ep Reward: -22.44, Avg(20): -35.97\n",
            "Ep: 232/600, Timesteps: 181802, LR: 2.10e-04, Ep Reward: -16.87, Avg(20): -35.34\n",
            "Ep: 233/600, Timesteps: 183402, LR: 2.09e-04, Ep Reward: -17.89, Avg(20): -34.79\n",
            "Ep: 234/600, Timesteps: 185002, LR: 2.08e-04, Ep Reward: -24.98, Avg(20): -34.43\n",
            "Ep: 235/600, Timesteps: 186602, LR: 2.07e-04, Ep Reward: -15.69, Avg(20): -29.73\n",
            "Ep: 236/600, Timesteps: 188202, LR: 2.07e-04, Ep Reward: -17.83, Avg(20): -29.09\n",
            "Ep: 237/600, Timesteps: 189802, LR: 2.06e-04, Ep Reward: -17.01, Avg(20): -24.33\n",
            "Ep: 238/600, Timesteps: 191402, LR: 2.05e-04, Ep Reward: -23.39, Avg(20): -24.06\n",
            "Ep: 239/600, Timesteps: 193002, LR: 2.04e-04, Ep Reward: -18.80, Avg(20): -23.50\n",
            "Ep: 240/600, Timesteps: 194602, LR: 2.03e-04, Ep Reward: -20.30, Avg(20): -23.10\n",
            "Ep: 241/600, Timesteps: 196202, LR: 2.03e-04, Ep Reward: -15.38, Avg(20): -22.38\n",
            "Ep: 242/600, Timesteps: 197802, LR: 2.02e-04, Ep Reward: -15.68, Avg(20): -21.93\n",
            "Ep: 243/600, Timesteps: 199402, LR: 2.01e-04, Ep Reward: -15.00, Avg(20): -21.50\n",
            "\n",
            "--- Checkpoint Model Saved: ppo_models/ppo_ac_BipedalWalker-v3_ckpt_200000.pth at Timestep 200000 ---\n",
            "Ep: 244/600, Timesteps: 201002, LR: 2.00e-04, Ep Reward: -18.41, Avg(20): -20.50\n",
            "Ep: 245/600, Timesteps: 201066, LR: 2.00e-04, Ep Reward: -99.00, Avg(20): -23.97\n",
            "Ep: 246/600, Timesteps: 202666, LR: 2.00e-04, Ep Reward: -16.02, Avg(20): -23.49\n",
            "Ep: 247/600, Timesteps: 204266, LR: 1.99e-04, Ep Reward: -16.20, Avg(20): -23.08\n",
            "Ep: 248/600, Timesteps: 205866, LR: 1.98e-04, Ep Reward: -22.43, Avg(20): -23.07\n",
            "Ep: 249/600, Timesteps: 207466, LR: 1.97e-04, Ep Reward: -14.10, Avg(20): -22.44\n",
            "Ep: 250/600, Timesteps: 209066, LR: 1.96e-04, Ep Reward: -15.65, Avg(20): -22.15\n",
            "Ep: 251/600, Timesteps: 209124, LR: 1.96e-04, Ep Reward: -100.33, Avg(20): -26.05\n",
            "Ep: 252/600, Timesteps: 210724, LR: 1.96e-04, Ep Reward: -13.86, Avg(20): -25.90\n",
            "Ep: 253/600, Timesteps: 212324, LR: 1.95e-04, Ep Reward: -17.85, Avg(20): -25.90\n",
            "Ep: 254/600, Timesteps: 213924, LR: 1.94e-04, Ep Reward: -14.90, Avg(20): -25.39\n",
            "Ep: 255/600, Timesteps: 215524, LR: 1.92e-04, Ep Reward: -16.79, Avg(20): -25.45\n",
            "Ep: 256/600, Timesteps: 217124, LR: 1.91e-04, Ep Reward: -16.59, Avg(20): -25.39\n",
            "Ep: 257/600, Timesteps: 218724, LR: 1.91e-04, Ep Reward: -13.26, Avg(20): -25.20\n",
            "Ep: 258/600, Timesteps: 220324, LR: 1.90e-04, Ep Reward: -18.90, Avg(20): -24.97\n",
            "Ep: 259/600, Timesteps: 221924, LR: 1.89e-04, Ep Reward: -12.62, Avg(20): -24.66\n",
            "Ep: 260/600, Timesteps: 221989, LR: 1.89e-04, Ep Reward: -113.37, Avg(20): -29.32\n",
            "Ep: 261/600, Timesteps: 223589, LR: 1.88e-04, Ep Reward: -12.50, Avg(20): -29.17\n",
            "Ep: 262/600, Timesteps: 225189, LR: 1.88e-04, Ep Reward: -11.08, Avg(20): -28.94\n",
            "Ep: 263/600, Timesteps: 226789, LR: 1.87e-04, Ep Reward: -12.65, Avg(20): -28.83\n",
            "Ep: 264/600, Timesteps: 228389, LR: 1.86e-04, Ep Reward: -13.39, Avg(20): -28.57\n",
            "Ep: 265/600, Timesteps: 229989, LR: 1.85e-04, Ep Reward: -12.48, Avg(20): -24.25\n",
            "Ep: 266/600, Timesteps: 231589, LR: 1.84e-04, Ep Reward: -13.04, Avg(20): -24.10\n",
            "Ep: 267/600, Timesteps: 233189, LR: 1.84e-04, Ep Reward: -11.00, Avg(20): -23.84\n",
            "Ep: 268/600, Timesteps: 234789, LR: 1.83e-04, Ep Reward: -14.34, Avg(20): -23.44\n",
            "Ep: 269/600, Timesteps: 236389, LR: 1.82e-04, Ep Reward: -10.07, Avg(20): -23.23\n",
            "Ep: 270/600, Timesteps: 237989, LR: 1.81e-04, Ep Reward: -12.11, Avg(20): -23.06\n",
            "Ep: 271/600, Timesteps: 239589, LR: 1.81e-04, Ep Reward: -12.83, Avg(20): -18.68\n",
            "Ep: 272/600, Timesteps: 241189, LR: 1.80e-04, Ep Reward: -15.05, Avg(20): -18.74\n",
            "Ep: 273/600, Timesteps: 242789, LR: 1.79e-04, Ep Reward: -11.39, Avg(20): -18.42\n",
            "Ep: 274/600, Timesteps: 244389, LR: 1.78e-04, Ep Reward: -12.33, Avg(20): -18.29\n",
            "Ep: 275/600, Timesteps: 245989, LR: 1.77e-04, Ep Reward: -13.87, Avg(20): -18.14\n",
            "Ep: 276/600, Timesteps: 247589, LR: 1.77e-04, Ep Reward: -9.37, Avg(20): -17.78\n",
            "Ep: 277/600, Timesteps: 247680, LR: 1.77e-04, Ep Reward: -115.97, Avg(20): -22.92\n",
            "Ep: 278/600, Timesteps: 249280, LR: 1.76e-04, Ep Reward: -10.87, Avg(20): -22.52\n",
            "Ep: 279/600, Timesteps: 250880, LR: 1.75e-04, Ep Reward: -13.29, Avg(20): -22.55\n",
            "Ep: 280/600, Timesteps: 252480, LR: 1.74e-04, Ep Reward: -60.23, Avg(20): -19.89\n",
            "Ep: 281/600, Timesteps: 254080, LR: 1.73e-04, Ep Reward: -15.07, Avg(20): -20.02\n",
            "Ep: 282/600, Timesteps: 255680, LR: 1.73e-04, Ep Reward: -12.09, Avg(20): -20.07\n",
            "Ep: 283/600, Timesteps: 257280, LR: 1.72e-04, Ep Reward: -14.94, Avg(20): -20.19\n",
            "Ep: 284/600, Timesteps: 258880, LR: 1.71e-04, Ep Reward: -15.76, Avg(20): -20.31\n",
            "Ep: 285/600, Timesteps: 260480, LR: 1.70e-04, Ep Reward: -16.35, Avg(20): -20.50\n",
            "Ep: 286/600, Timesteps: 262080, LR: 1.70e-04, Ep Reward: -13.03, Avg(20): -20.50\n",
            "Ep: 287/600, Timesteps: 263680, LR: 1.69e-04, Ep Reward: -12.20, Avg(20): -20.56\n",
            "Ep: 288/600, Timesteps: 265280, LR: 1.68e-04, Ep Reward: -19.10, Avg(20): -20.80\n",
            "Ep: 289/600, Timesteps: 266880, LR: 1.67e-04, Ep Reward: -18.51, Avg(20): -21.22\n",
            "Ep: 290/600, Timesteps: 268480, LR: 1.66e-04, Ep Reward: -12.42, Avg(20): -21.23\n",
            "Ep: 291/600, Timesteps: 270080, LR: 1.66e-04, Ep Reward: -15.65, Avg(20): -21.37\n",
            "Ep: 292/600, Timesteps: 271680, LR: 1.65e-04, Ep Reward: -14.10, Avg(20): -21.33\n",
            "Ep: 293/600, Timesteps: 273280, LR: 1.64e-04, Ep Reward: -15.49, Avg(20): -21.53\n",
            "Ep: 294/600, Timesteps: 274880, LR: 1.63e-04, Ep Reward: -17.66, Avg(20): -21.80\n",
            "Ep: 295/600, Timesteps: 276480, LR: 1.63e-04, Ep Reward: -11.76, Avg(20): -21.69\n",
            "Ep: 296/600, Timesteps: 278080, LR: 1.62e-04, Ep Reward: -16.92, Avg(20): -22.07\n",
            "Ep: 297/600, Timesteps: 278144, LR: 1.62e-04, Ep Reward: -101.07, Avg(20): -21.33\n",
            "Ep: 298/600, Timesteps: 279744, LR: 1.61e-04, Ep Reward: -13.17, Avg(20): -21.44\n",
            "Ep: 299/600, Timesteps: 281344, LR: 1.60e-04, Ep Reward: -15.18, Avg(20): -21.54\n",
            "Ep: 300/600, Timesteps: 282944, LR: 1.59e-04, Ep Reward: -16.18, Avg(20): -19.33\n",
            "Ep: 301/600, Timesteps: 283046, LR: 1.59e-04, Ep Reward: -93.47, Avg(20): -23.25\n",
            "Ep: 302/600, Timesteps: 284646, LR: 1.59e-04, Ep Reward: -13.13, Avg(20): -23.30\n",
            "Ep: 303/600, Timesteps: 284719, LR: 1.58e-04, Ep Reward: -98.35, Avg(20): -27.48\n",
            "Ep: 304/600, Timesteps: 286319, LR: 1.58e-04, Ep Reward: -11.73, Avg(20): -27.27\n",
            "Ep: 305/600, Timesteps: 287919, LR: 1.57e-04, Ep Reward: -14.25, Avg(20): -27.17\n",
            "Ep: 306/600, Timesteps: 289519, LR: 1.56e-04, Ep Reward: -12.48, Avg(20): -27.14\n",
            "Ep: 307/600, Timesteps: 291119, LR: 1.55e-04, Ep Reward: -12.71, Avg(20): -27.17\n",
            "Ep: 308/600, Timesteps: 292719, LR: 1.55e-04, Ep Reward: -16.11, Avg(20): -27.02\n",
            "Ep: 309/600, Timesteps: 294319, LR: 1.54e-04, Ep Reward: -11.76, Avg(20): -26.68\n",
            "Ep: 310/600, Timesteps: 295919, LR: 1.53e-04, Ep Reward: -20.00, Avg(20): -27.06\n",
            "Ep: 311/600, Timesteps: 297519, LR: 1.52e-04, Ep Reward: -14.03, Avg(20): -26.98\n",
            "Ep: 312/600, Timesteps: 299119, LR: 1.50e-04, Ep Reward: -11.39, Avg(20): -26.84\n",
            "\n",
            "--- Checkpoint Model Saved: ppo_models/ppo_ac_BipedalWalker-v3_ckpt_300000.pth at Timestep 300000 ---\n",
            "Ep: 313/600, Timesteps: 300719, LR: 1.50e-04, Ep Reward: -17.90, Avg(20): -26.96\n",
            "Ep: 314/600, Timesteps: 302319, LR: 1.49e-04, Ep Reward: -14.46, Avg(20): -26.80\n",
            "Ep: 315/600, Timesteps: 303919, LR: 1.48e-04, Ep Reward: -14.24, Avg(20): -26.93\n",
            "Ep: 316/600, Timesteps: 305519, LR: 1.47e-04, Ep Reward: -14.54, Avg(20): -26.81\n",
            "Ep: 317/600, Timesteps: 307119, LR: 1.47e-04, Ep Reward: -18.50, Avg(20): -22.68\n",
            "Ep: 318/600, Timesteps: 307243, LR: 1.46e-04, Ep Reward: -105.62, Avg(20): -27.30\n",
            "Ep: 319/600, Timesteps: 308843, LR: 1.46e-04, Ep Reward: -21.66, Avg(20): -27.63\n",
            "Ep: 320/600, Timesteps: 310443, LR: 1.45e-04, Ep Reward: -15.22, Avg(20): -27.58\n",
            "Ep: 321/600, Timesteps: 310517, LR: 1.45e-04, Ep Reward: -114.54, Avg(20): -28.63\n",
            "Ep: 322/600, Timesteps: 312117, LR: 1.44e-04, Ep Reward: -16.39, Avg(20): -28.79\n",
            "Ep: 323/600, Timesteps: 313717, LR: 1.43e-04, Ep Reward: -18.16, Avg(20): -24.78\n",
            "Ep: 324/600, Timesteps: 315317, LR: 1.43e-04, Ep Reward: -20.46, Avg(20): -25.22\n",
            "Ep: 325/600, Timesteps: 315395, LR: 1.42e-04, Ep Reward: -99.17, Avg(20): -29.47\n",
            "Ep: 326/600, Timesteps: 316995, LR: 1.42e-04, Ep Reward: -13.15, Avg(20): -29.50\n",
            "Ep: 327/600, Timesteps: 318595, LR: 1.41e-04, Ep Reward: -18.57, Avg(20): -29.79\n",
            "Ep: 328/600, Timesteps: 320195, LR: 1.40e-04, Ep Reward: -12.02, Avg(20): -29.59\n",
            "Ep: 329/600, Timesteps: 321795, LR: 1.39e-04, Ep Reward: -13.37, Avg(20): -29.67\n",
            "Ep: 330/600, Timesteps: 323395, LR: 1.39e-04, Ep Reward: -15.77, Avg(20): -29.46\n",
            "Ep: 331/600, Timesteps: 324995, LR: 1.38e-04, Ep Reward: -14.18, Avg(20): -29.46\n",
            "Ep: 332/600, Timesteps: 326595, LR: 1.37e-04, Ep Reward: -11.89, Avg(20): -29.49\n",
            "Ep: 333/600, Timesteps: 326702, LR: 1.37e-04, Ep Reward: -102.98, Avg(20): -33.74\n",
            "Ep: 334/600, Timesteps: 328302, LR: 1.36e-04, Ep Reward: -14.07, Avg(20): -33.72\n",
            "Ep: 335/600, Timesteps: 329902, LR: 1.35e-04, Ep Reward: -16.10, Avg(20): -33.82\n",
            "Ep: 336/600, Timesteps: 331502, LR: 1.35e-04, Ep Reward: -11.73, Avg(20): -33.68\n",
            "Ep: 337/600, Timesteps: 333102, LR: 1.34e-04, Ep Reward: -15.48, Avg(20): -33.53\n",
            "Ep: 338/600, Timesteps: 334702, LR: 1.33e-04, Ep Reward: -10.50, Avg(20): -28.77\n",
            "Ep: 339/600, Timesteps: 336302, LR: 1.32e-04, Ep Reward: -8.87, Avg(20): -28.13\n",
            "Ep: 340/600, Timesteps: 337902, LR: 1.32e-04, Ep Reward: -20.64, Avg(20): -28.40\n",
            "Ep: 341/600, Timesteps: 337969, LR: 1.31e-04, Ep Reward: -102.10, Avg(20): -27.78\n",
            "Ep: 342/600, Timesteps: 339569, LR: 1.31e-04, Ep Reward: -9.21, Avg(20): -27.42\n",
            "Ep: 343/600, Timesteps: 341169, LR: 1.30e-04, Ep Reward: -14.26, Avg(20): -27.23\n",
            "Ep: 344/600, Timesteps: 342769, LR: 1.29e-04, Ep Reward: -17.32, Avg(20): -27.07\n",
            "Ep: 345/600, Timesteps: 342827, LR: 1.29e-04, Ep Reward: -116.98, Avg(20): -27.96\n",
            "Ep: 346/600, Timesteps: 344427, LR: 1.28e-04, Ep Reward: -10.26, Avg(20): -27.81\n",
            "Ep: 347/600, Timesteps: 346027, LR: 1.28e-04, Ep Reward: -18.60, Avg(20): -27.82\n",
            "Ep: 348/600, Timesteps: 347627, LR: 1.27e-04, Ep Reward: -16.47, Avg(20): -28.04\n",
            "Ep: 349/600, Timesteps: 349227, LR: 1.26e-04, Ep Reward: -12.33, Avg(20): -27.99\n",
            "Ep: 350/600, Timesteps: 350827, LR: 1.25e-04, Ep Reward: -12.00, Avg(20): -27.80\n",
            "Ep: 351/600, Timesteps: 350923, LR: 1.25e-04, Ep Reward: -121.97, Avg(20): -33.19\n",
            "Ep: 352/600, Timesteps: 352523, LR: 1.24e-04, Ep Reward: -14.23, Avg(20): -33.30\n",
            "Ep: 353/600, Timesteps: 354123, LR: 1.24e-04, Ep Reward: -12.25, Avg(20): -28.77\n",
            "Ep: 354/600, Timesteps: 355723, LR: 1.23e-04, Ep Reward: -11.53, Avg(20): -28.64\n",
            "Ep: 355/600, Timesteps: 357323, LR: 1.22e-04, Ep Reward: -8.63, Avg(20): -28.27\n",
            "Ep: 356/600, Timesteps: 358923, LR: 1.21e-04, Ep Reward: -6.70, Avg(20): -28.02\n",
            "Ep: 357/600, Timesteps: 360523, LR: 1.20e-04, Ep Reward: -10.60, Avg(20): -27.77\n",
            "Ep: 358/600, Timesteps: 362123, LR: 1.20e-04, Ep Reward: -8.12, Avg(20): -27.65\n",
            "Ep: 359/600, Timesteps: 363723, LR: 1.19e-04, Ep Reward: -10.77, Avg(20): -27.75\n",
            "Ep: 360/600, Timesteps: 365323, LR: 1.18e-04, Ep Reward: -9.14, Avg(20): -27.17\n",
            "Ep: 361/600, Timesteps: 366923, LR: 1.17e-04, Ep Reward: -6.81, Avg(20): -22.41\n",
            "Ep: 362/600, Timesteps: 368523, LR: 1.17e-04, Ep Reward: -9.16, Avg(20): -22.41\n",
            "Ep: 363/600, Timesteps: 370123, LR: 1.16e-04, Ep Reward: -4.49, Avg(20): -21.92\n",
            "Ep: 364/600, Timesteps: 371723, LR: 1.15e-04, Ep Reward: -9.76, Avg(20): -21.54\n",
            "Ep: 365/600, Timesteps: 373323, LR: 1.14e-04, Ep Reward: -4.41, Avg(20): -15.91\n",
            "Ep: 366/600, Timesteps: 374923, LR: 1.13e-04, Ep Reward: -4.01, Avg(20): -15.60\n",
            "Ep: 367/600, Timesteps: 376523, LR: 1.13e-04, Ep Reward: -5.51, Avg(20): -14.94\n",
            "Ep: 368/600, Timesteps: 378123, LR: 1.12e-04, Ep Reward: -4.92, Avg(20): -14.37\n",
            "Ep: 369/600, Timesteps: 379723, LR: 1.11e-04, Ep Reward: -9.66, Avg(20): -14.23\n",
            "Ep: 370/600, Timesteps: 381323, LR: 1.10e-04, Ep Reward: -6.35, Avg(20): -13.95\n",
            "Ep: 371/600, Timesteps: 382923, LR: 1.10e-04, Ep Reward: -12.53, Avg(20): -8.48\n",
            "Ep: 372/600, Timesteps: 384523, LR: 1.09e-04, Ep Reward: -10.23, Avg(20): -8.28\n",
            "Ep: 373/600, Timesteps: 386123, LR: 1.07e-04, Ep Reward: -9.54, Avg(20): -8.14\n",
            "Ep: 374/600, Timesteps: 387723, LR: 1.06e-04, Ep Reward: -5.81, Avg(20): -7.86\n",
            "Ep: 375/600, Timesteps: 389323, LR: 1.05e-04, Ep Reward: -4.91, Avg(20): -7.67\n",
            "Ep: 376/600, Timesteps: 390923, LR: 1.05e-04, Ep Reward: -77.82, Avg(20): -11.23\n",
            "Ep: 377/600, Timesteps: 392523, LR: 1.04e-04, Ep Reward: -8.10, Avg(20): -11.10\n",
            "Ep: 378/600, Timesteps: 394123, LR: 1.03e-04, Ep Reward: -7.66, Avg(20): -11.08\n",
            "Ep: 379/600, Timesteps: 395723, LR: 1.02e-04, Ep Reward: -5.43, Avg(20): -10.81\n",
            "Ep: 380/600, Timesteps: 397323, LR: 1.01e-04, Ep Reward: -7.21, Avg(20): -10.72\n",
            "Ep: 381/600, Timesteps: 398923, LR: 1.01e-04, Ep Reward: -3.00, Avg(20): -10.52\n",
            "\n",
            "--- Checkpoint Model Saved: ppo_models/ppo_ac_BipedalWalker-v3_ckpt_400000.pth at Timestep 400000 ---\n",
            "Ep: 382/600, Timesteps: 400523, LR: 1.00e-04, Ep Reward: -10.88, Avg(20): -10.61\n",
            "Ep: 383/600, Timesteps: 402123, LR: 9.93e-05, Ep Reward: -3.17, Avg(20): -10.54\n",
            "Ep: 384/600, Timesteps: 402232, LR: 9.93e-05, Ep Reward: -103.73, Avg(20): -15.24\n",
            "Ep: 385/600, Timesteps: 403832, LR: 9.83e-05, Ep Reward: -3.24, Avg(20): -15.18\n",
            "Ep: 386/600, Timesteps: 405432, LR: 9.83e-05, Ep Reward: -6.67, Avg(20): -15.32\n",
            "Ep: 387/600, Timesteps: 407032, LR: 9.72e-05, Ep Reward: -7.91, Avg(20): -15.44\n",
            "Ep: 388/600, Timesteps: 408632, LR: 9.62e-05, Ep Reward: -6.23, Avg(20): -15.50\n",
            "Ep: 389/600, Timesteps: 410232, LR: 9.52e-05, Ep Reward: -8.02, Avg(20): -15.42\n",
            "Ep: 390/600, Timesteps: 411832, LR: 9.42e-05, Ep Reward: -6.62, Avg(20): -15.43\n",
            "Ep: 391/600, Timesteps: 413432, LR: 9.42e-05, Ep Reward: -6.04, Avg(20): -15.11\n",
            "Ep: 392/600, Timesteps: 415032, LR: 9.32e-05, Ep Reward: -5.66, Avg(20): -14.88\n",
            "Ep: 393/600, Timesteps: 416632, LR: 9.21e-05, Ep Reward: -4.88, Avg(20): -14.65\n",
            "Ep: 394/600, Timesteps: 418232, LR: 9.11e-05, Ep Reward: -10.44, Avg(20): -14.88\n",
            "Ep: 395/600, Timesteps: 419832, LR: 9.11e-05, Ep Reward: -4.79, Avg(20): -14.87\n",
            "Ep: 396/600, Timesteps: 421432, LR: 9.01e-05, Ep Reward: -3.45, Avg(20): -11.16\n",
            "Ep: 397/600, Timesteps: 423032, LR: 8.91e-05, Ep Reward: -7.32, Avg(20): -11.12\n",
            "Ep: 398/600, Timesteps: 424632, LR: 8.80e-05, Ep Reward: -5.64, Avg(20): -11.02\n",
            "Ep: 399/600, Timesteps: 426232, LR: 8.70e-05, Ep Reward: -13.98, Avg(20): -11.44\n",
            "Ep: 400/600, Timesteps: 427832, LR: 8.70e-05, Ep Reward: -7.06, Avg(20): -11.44\n",
            "Ep: 401/600, Timesteps: 429432, LR: 8.60e-05, Ep Reward: -7.48, Avg(20): -11.66\n",
            "Ep: 402/600, Timesteps: 429559, LR: 8.60e-05, Ep Reward: -97.98, Avg(20): -16.02\n",
            "Ep: 403/600, Timesteps: 431159, LR: 8.50e-05, Ep Reward: -9.23, Avg(20): -16.32\n",
            "Ep: 404/600, Timesteps: 432759, LR: 8.39e-05, Ep Reward: -7.56, Avg(20): -11.51\n",
            "Ep: 405/600, Timesteps: 434359, LR: 8.29e-05, Ep Reward: -11.09, Avg(20): -11.90\n",
            "Ep: 406/600, Timesteps: 434558, LR: 8.29e-05, Ep Reward: -110.28, Avg(20): -17.08\n",
            "Ep: 407/600, Timesteps: 436158, LR: 8.29e-05, Ep Reward: -53.57, Avg(20): -19.37\n",
            "Ep: 408/600, Timesteps: 436713, LR: 8.19e-05, Ep Reward: -118.55, Avg(20): -24.98\n",
            "Ep: 409/600, Timesteps: 438313, LR: 8.09e-05, Ep Reward: -5.68, Avg(20): -24.86\n",
            "Ep: 410/600, Timesteps: 438352, LR: 8.09e-05, Ep Reward: -108.90, Avg(20): -29.98\n",
            "Ep: 411/600, Timesteps: 438435, LR: 8.09e-05, Ep Reward: -102.64, Avg(20): -34.81\n",
            "Ep: 412/600, Timesteps: 440035, LR: 8.09e-05, Ep Reward: -2.69, Avg(20): -34.66\n",
            "Ep: 413/600, Timesteps: 441635, LR: 7.98e-05, Ep Reward: -19.90, Avg(20): -35.41\n",
            "Ep: 414/600, Timesteps: 443235, LR: 7.88e-05, Ep Reward: 0.16, Avg(20): -34.88\n",
            "Ep: 415/600, Timesteps: 443332, LR: 7.88e-05, Ep Reward: -105.25, Avg(20): -39.90\n",
            "Ep: 416/600, Timesteps: 443444, LR: 7.88e-05, Ep Reward: -101.93, Avg(20): -44.83\n",
            "Ep: 417/600, Timesteps: 445044, LR: 7.78e-05, Ep Reward: -3.49, Avg(20): -44.64\n",
            "Ep: 418/600, Timesteps: 446644, LR: 7.68e-05, Ep Reward: -4.40, Avg(20): -44.57\n",
            "Ep: 419/600, Timesteps: 448244, LR: 7.68e-05, Ep Reward: -4.15, Avg(20): -44.08\n",
            "Ep: 420/600, Timesteps: 448351, LR: 7.68e-05, Ep Reward: -117.62, Avg(20): -49.61\n",
            "Ep: 421/600, Timesteps: 449951, LR: 7.57e-05, Ep Reward: -0.83, Avg(20): -49.28\n",
            "Ep: 422/600, Timesteps: 451551, LR: 7.47e-05, Ep Reward: -4.55, Avg(20): -44.61\n",
            "Ep: 423/600, Timesteps: 453151, LR: 7.37e-05, Ep Reward: -0.63, Avg(20): -44.18\n",
            "Ep: 424/600, Timesteps: 453245, LR: 7.37e-05, Ep Reward: -103.36, Avg(20): -48.97\n",
            "Ep: 425/600, Timesteps: 454845, LR: 7.27e-05, Ep Reward: -13.92, Avg(20): -49.11\n",
            "Ep: 426/600, Timesteps: 454940, LR: 7.27e-05, Ep Reward: -102.25, Avg(20): -48.71\n",
            "Ep: 427/600, Timesteps: 456540, LR: 7.27e-05, Ep Reward: -9.62, Avg(20): -46.51\n",
            "Ep: 428/600, Timesteps: 458140, LR: 7.16e-05, Ep Reward: -4.76, Avg(20): -40.82\n",
            "Ep: 429/600, Timesteps: 459740, LR: 7.06e-05, Ep Reward: -2.07, Avg(20): -40.64\n",
            "Ep: 430/600, Timesteps: 459793, LR: 7.06e-05, Ep Reward: -112.50, Avg(20): -40.82\n",
            "Ep: 431/600, Timesteps: 461393, LR: 6.96e-05, Ep Reward: -3.46, Avg(20): -35.86\n",
            "Ep: 432/600, Timesteps: 462993, LR: 6.86e-05, Ep Reward: -5.18, Avg(20): -35.98\n",
            "Ep: 433/600, Timesteps: 464593, LR: 6.86e-05, Ep Reward: -3.14, Avg(20): -35.15\n",
            "Ep: 434/600, Timesteps: 466193, LR: 6.76e-05, Ep Reward: -4.74, Avg(20): -35.39\n",
            "Ep: 435/600, Timesteps: 467793, LR: 6.65e-05, Ep Reward: -5.01, Avg(20): -30.38\n",
            "Ep: 436/600, Timesteps: 469393, LR: 6.55e-05, Ep Reward: -4.29, Avg(20): -25.50\n",
            "Ep: 437/600, Timesteps: 470993, LR: 6.55e-05, Ep Reward: -0.77, Avg(20): -25.36\n",
            "Ep: 438/600, Timesteps: 472593, LR: 6.45e-05, Ep Reward: -2.30, Avg(20): -25.26\n",
            "Ep: 439/600, Timesteps: 472711, LR: 6.45e-05, Ep Reward: -118.70, Avg(20): -30.98\n",
            "Ep: 440/600, Timesteps: 472809, LR: 6.45e-05, Ep Reward: -106.59, Avg(20): -30.43\n",
            "Ep: 441/600, Timesteps: 474409, LR: 6.35e-05, Ep Reward: 0.08, Avg(20): -30.39\n",
            "Ep: 442/600, Timesteps: 476009, LR: 6.24e-05, Ep Reward: -10.91, Avg(20): -30.71\n",
            "Ep: 443/600, Timesteps: 477609, LR: 6.14e-05, Ep Reward: -8.77, Avg(20): -31.11\n",
            "Ep: 444/600, Timesteps: 479209, LR: 6.14e-05, Ep Reward: -2.34, Avg(20): -26.06\n",
            "Ep: 445/600, Timesteps: 480809, LR: 6.04e-05, Ep Reward: -1.64, Avg(20): -25.45\n",
            "Ep: 446/600, Timesteps: 482409, LR: 5.94e-05, Ep Reward: -3.61, Avg(20): -20.52\n",
            "Ep: 447/600, Timesteps: 482457, LR: 5.94e-05, Ep Reward: -114.80, Avg(20): -25.77\n",
            "Ep: 448/600, Timesteps: 482584, LR: 5.94e-05, Ep Reward: -97.91, Avg(20): -30.43\n",
            "Ep: 449/600, Timesteps: 484184, LR: 5.83e-05, Ep Reward: -3.52, Avg(20): -30.50\n",
            "Ep: 450/600, Timesteps: 485784, LR: 5.73e-05, Ep Reward: -2.66, Avg(20): -25.01\n",
            "Ep: 451/600, Timesteps: 487384, LR: 5.73e-05, Ep Reward: -2.72, Avg(20): -24.98\n",
            "Ep: 452/600, Timesteps: 488984, LR: 5.63e-05, Ep Reward: -7.54, Avg(20): -25.09\n",
            "Ep: 453/600, Timesteps: 490584, LR: 5.53e-05, Ep Reward: -3.32, Avg(20): -25.10\n",
            "Ep: 454/600, Timesteps: 492184, LR: 5.42e-05, Ep Reward: -8.61, Avg(20): -25.30\n",
            "Ep: 455/600, Timesteps: 493784, LR: 5.32e-05, Ep Reward: -8.41, Avg(20): -25.47\n",
            "Ep: 456/600, Timesteps: 495384, LR: 5.32e-05, Ep Reward: -3.94, Avg(20): -25.45\n",
            "Ep: 457/600, Timesteps: 496984, LR: 5.22e-05, Ep Reward: -8.92, Avg(20): -25.86\n",
            "Ep: 458/600, Timesteps: 498584, LR: 5.12e-05, Ep Reward: -6.45, Avg(20): -26.06\n",
            "\n",
            "--- Checkpoint Model Saved: ppo_models/ppo_ac_BipedalWalker-v3_ckpt_500000.pth at Timestep 500000 ---\n",
            "Ep: 459/600, Timesteps: 500184, LR: 5.01e-05, Ep Reward: -1.09, Avg(20): -20.18\n",
            "Ep: 460/600, Timesteps: 501784, LR: 4.91e-05, Ep Reward: -10.16, Avg(20): -15.36\n",
            "Ep: 461/600, Timesteps: 503384, LR: 4.91e-05, Ep Reward: -12.04, Avg(20): -15.97\n",
            "Ep: 462/600, Timesteps: 504984, LR: 4.81e-05, Ep Reward: -0.86, Avg(20): -15.47\n",
            "Ep: 463/600, Timesteps: 506584, LR: 4.71e-05, Ep Reward: -1.28, Avg(20): -15.09\n",
            "Ep: 464/600, Timesteps: 508184, LR: 4.60e-05, Ep Reward: -5.83, Avg(20): -15.26\n",
            "Ep: 465/600, Timesteps: 509784, LR: 4.60e-05, Ep Reward: 1.57, Avg(20): -15.10\n",
            "Ep: 466/600, Timesteps: 509832, LR: 4.60e-05, Ep Reward: -108.17, Avg(20): -20.33\n",
            "Ep: 467/600, Timesteps: 511432, LR: 4.50e-05, Ep Reward: -3.89, Avg(20): -14.79\n",
            "Ep: 468/600, Timesteps: 513032, LR: 4.40e-05, Ep Reward: -2.70, Avg(20): -10.03\n",
            "Ep: 469/600, Timesteps: 514632, LR: 4.30e-05, Ep Reward: -3.74, Avg(20): -10.04\n",
            "Ep: 470/600, Timesteps: 516232, LR: 4.20e-05, Ep Reward: -3.21, Avg(20): -10.06\n",
            "Ep: 471/600, Timesteps: 517832, LR: 4.20e-05, Ep Reward: -9.79, Avg(20): -10.42\n",
            "Ep: 472/600, Timesteps: 517890, LR: 4.20e-05, Ep Reward: -117.46, Avg(20): -15.91\n",
            "Ep: 473/600, Timesteps: 519490, LR: 4.09e-05, Ep Reward: -2.99, Avg(20): -15.90\n",
            "Ep: 474/600, Timesteps: 521090, LR: 3.99e-05, Ep Reward: -5.03, Avg(20): -15.72\n",
            "Ep: 475/600, Timesteps: 522690, LR: 3.89e-05, Ep Reward: -4.62, Avg(20): -15.53\n",
            "Ep: 476/600, Timesteps: 524290, LR: 3.79e-05, Ep Reward: 0.05, Avg(20): -15.33\n",
            "Ep: 477/600, Timesteps: 525890, LR: 3.79e-05, Ep Reward: -2.15, Avg(20): -14.99\n",
            "Ep: 478/600, Timesteps: 527490, LR: 3.68e-05, Ep Reward: -6.61, Avg(20): -15.00\n",
            "Ep: 479/600, Timesteps: 529090, LR: 3.58e-05, Ep Reward: -2.18, Avg(20): -15.05\n",
            "Ep: 480/600, Timesteps: 530690, LR: 3.48e-05, Ep Reward: -7.99, Avg(20): -14.95\n",
            "Ep: 481/600, Timesteps: 532290, LR: 3.48e-05, Ep Reward: -2.78, Avg(20): -14.48\n",
            "Ep: 482/600, Timesteps: 533890, LR: 3.38e-05, Ep Reward: -4.11, Avg(20): -14.64\n",
            "Ep: 483/600, Timesteps: 535490, LR: 3.27e-05, Ep Reward: -4.04, Avg(20): -14.78\n",
            "Ep: 484/600, Timesteps: 537090, LR: 3.17e-05, Ep Reward: 1.32, Avg(20): -14.43\n",
            "Ep: 485/600, Timesteps: 538690, LR: 3.07e-05, Ep Reward: -16.20, Avg(20): -15.31\n",
            "Ep: 486/600, Timesteps: 540290, LR: 3.07e-05, Ep Reward: -2.31, Avg(20): -10.02\n",
            "Ep: 487/600, Timesteps: 541890, LR: 2.97e-05, Ep Reward: -11.41, Avg(20): -10.40\n",
            "Ep: 488/600, Timesteps: 543490, LR: 2.86e-05, Ep Reward: -3.52, Avg(20): -10.44\n",
            "Ep: 489/600, Timesteps: 545090, LR: 2.76e-05, Ep Reward: -8.35, Avg(20): -10.67\n",
            "Ep: 490/600, Timesteps: 546690, LR: 2.76e-05, Ep Reward: -11.49, Avg(20): -11.08\n",
            "Ep: 491/600, Timesteps: 548290, LR: 2.66e-05, Ep Reward: -2.30, Avg(20): -10.71\n",
            "Ep: 492/600, Timesteps: 549890, LR: 2.56e-05, Ep Reward: -3.45, Avg(20): -5.01\n",
            "Ep: 493/600, Timesteps: 551490, LR: 2.45e-05, Ep Reward: -6.37, Avg(20): -5.18\n",
            "Ep: 494/600, Timesteps: 553090, LR: 2.35e-05, Ep Reward: -8.87, Avg(20): -5.37\n",
            "Ep: 495/600, Timesteps: 554690, LR: 2.35e-05, Ep Reward: -4.57, Avg(20): -5.37\n",
            "Ep: 496/600, Timesteps: 556290, LR: 2.25e-05, Ep Reward: -4.85, Avg(20): -5.61\n",
            "Ep: 497/600, Timesteps: 557890, LR: 2.15e-05, Ep Reward: -1.43, Avg(20): -5.57\n",
            "Ep: 498/600, Timesteps: 559490, LR: 2.04e-05, Ep Reward: -10.12, Avg(20): -5.75\n",
            "Ep: 499/600, Timesteps: 561090, LR: 2.04e-05, Ep Reward: -3.37, Avg(20): -5.81\n",
            "Ep: 500/600, Timesteps: 562690, LR: 1.94e-05, Ep Reward: 1.05, Avg(20): -5.36\n",
            "Ep: 501/600, Timesteps: 564290, LR: 1.84e-05, Ep Reward: -3.94, Avg(20): -5.42\n",
            "Ep: 502/600, Timesteps: 565890, LR: 1.74e-05, Ep Reward: -12.16, Avg(20): -5.82\n",
            "Ep: 503/600, Timesteps: 567490, LR: 1.64e-05, Ep Reward: -3.91, Avg(20): -5.81\n",
            "Ep: 504/600, Timesteps: 569090, LR: 1.64e-05, Ep Reward: -10.87, Avg(20): -6.42\n",
            "Ep: 505/600, Timesteps: 570690, LR: 1.53e-05, Ep Reward: -0.33, Avg(20): -5.63\n",
            "Ep: 506/600, Timesteps: 572290, LR: 1.43e-05, Ep Reward: -0.32, Avg(20): -5.53\n",
            "Ep: 507/600, Timesteps: 573890, LR: 1.33e-05, Ep Reward: -3.10, Avg(20): -5.11\n",
            "Ep: 508/600, Timesteps: 575490, LR: 1.23e-05, Ep Reward: -8.70, Avg(20): -5.37\n",
            "Ep: 509/600, Timesteps: 575605, LR: 1.23e-05, Ep Reward: -117.05, Avg(20): -10.81\n",
            "Ep: 510/600, Timesteps: 577205, LR: 1.23e-05, Ep Reward: -13.88, Avg(20): -10.93\n",
            "Ep: 511/600, Timesteps: 578805, LR: 1.12e-05, Ep Reward: -2.51, Avg(20): -10.94\n",
            "Ep: 512/600, Timesteps: 580405, LR: 1.02e-05, Ep Reward: -3.48, Avg(20): -10.94\n",
            "Ep: 513/600, Timesteps: 582005, LR: 9.18e-06, Ep Reward: -4.91, Avg(20): -10.87\n",
            "Ep: 514/600, Timesteps: 583605, LR: 9.18e-06, Ep Reward: -4.49, Avg(20): -10.65\n",
            "Ep: 515/600, Timesteps: 585205, LR: 8.16e-06, Ep Reward: -10.24, Avg(20): -10.93\n",
            "Ep: 516/600, Timesteps: 586805, LR: 7.14e-06, Ep Reward: -3.05, Avg(20): -10.84\n",
            "Ep: 517/600, Timesteps: 588405, LR: 6.11e-06, Ep Reward: -8.81, Avg(20): -11.21\n",
            "Ep: 518/600, Timesteps: 590005, LR: 5.09e-06, Ep Reward: -0.03, Avg(20): -10.70\n",
            "Ep: 519/600, Timesteps: 591605, LR: 5.09e-06, Ep Reward: -7.32, Avg(20): -10.90\n",
            "Ep: 520/600, Timesteps: 593205, LR: 4.06e-06, Ep Reward: -2.59, Avg(20): -11.08\n",
            "Ep: 521/600, Timesteps: 594805, LR: 3.04e-06, Ep Reward: -6.09, Avg(20): -11.19\n",
            "Ep: 522/600, Timesteps: 596405, LR: 2.02e-06, Ep Reward: -7.53, Avg(20): -10.96\n",
            "Ep: 523/600, Timesteps: 598005, LR: 2.02e-06, Ep Reward: -1.22, Avg(20): -10.83\n",
            "Ep: 524/600, Timesteps: 599605, LR: 9.92e-07, Ep Reward: -3.80, Avg(20): -10.47\n",
            "\n",
            "--- Checkpoint Model Saved: ppo_models/ppo_ac_BipedalWalker-v3_ckpt_600000.pth at Timestep 600000 ---\n",
            "Ep: 525/600, Timesteps: 601205, LR: 0.00e+00, Ep Reward: -14.51, Avg(20): -11.18\n",
            "Ep: 526/600, Timesteps: 602805, LR: 0.00e+00, Ep Reward: -8.40, Avg(20): -11.59\n",
            "Ep: 527/600, Timesteps: 602859, LR: 0.00e+00, Ep Reward: -108.61, Avg(20): -16.86\n",
            "Ep: 528/600, Timesteps: 602919, LR: 0.00e+00, Ep Reward: -110.12, Avg(20): -21.93\n",
            "Ep: 529/600, Timesteps: 604519, LR: 0.00e+00, Ep Reward: -1.47, Avg(20): -16.15\n",
            "Ep: 530/600, Timesteps: 606119, LR: 0.00e+00, Ep Reward: -3.42, Avg(20): -15.63\n",
            "Ep: 531/600, Timesteps: 607719, LR: 0.00e+00, Ep Reward: -12.99, Avg(20): -16.15\n",
            "Ep: 532/600, Timesteps: 609319, LR: 0.00e+00, Ep Reward: -1.30, Avg(20): -16.05\n",
            "Ep: 533/600, Timesteps: 610919, LR: 0.00e+00, Ep Reward: -0.62, Avg(20): -15.83\n",
            "Ep: 534/600, Timesteps: 612519, LR: 0.00e+00, Ep Reward: -5.11, Avg(20): -15.86\n",
            "Ep: 535/600, Timesteps: 614119, LR: 0.00e+00, Ep Reward: -6.45, Avg(20): -15.67\n",
            "Ep: 536/600, Timesteps: 615719, LR: 0.00e+00, Ep Reward: -6.13, Avg(20): -15.83\n",
            "Ep: 537/600, Timesteps: 617319, LR: 0.00e+00, Ep Reward: -6.43, Avg(20): -15.71\n",
            "Ep: 538/600, Timesteps: 618919, LR: 0.00e+00, Ep Reward: -3.48, Avg(20): -15.88\n",
            "Ep: 539/600, Timesteps: 620519, LR: 0.00e+00, Ep Reward: -6.39, Avg(20): -15.83\n",
            "Ep: 540/600, Timesteps: 622119, LR: 0.00e+00, Ep Reward: -0.94, Avg(20): -15.75\n",
            "Ep: 541/600, Timesteps: 623719, LR: 0.00e+00, Ep Reward: -2.84, Avg(20): -15.59\n",
            "Ep: 542/600, Timesteps: 625319, LR: 0.00e+00, Ep Reward: -3.92, Avg(20): -15.41\n",
            "Ep: 543/600, Timesteps: 626919, LR: 0.00e+00, Ep Reward: -2.27, Avg(20): -15.46\n",
            "Ep: 544/600, Timesteps: 628519, LR: 0.00e+00, Ep Reward: 0.49, Avg(20): -15.25\n",
            "Ep: 545/600, Timesteps: 630119, LR: 0.00e+00, Ep Reward: -1.33, Avg(20): -14.59\n",
            "Ep: 546/600, Timesteps: 631719, LR: 0.00e+00, Ep Reward: -4.29, Avg(20): -14.38\n",
            "Ep: 547/600, Timesteps: 633319, LR: 0.00e+00, Ep Reward: -4.19, Avg(20): -9.16\n",
            "Ep: 548/600, Timesteps: 634919, LR: 0.00e+00, Ep Reward: -3.70, Avg(20): -3.84\n",
            "Ep: 549/600, Timesteps: 636519, LR: 0.00e+00, Ep Reward: -1.76, Avg(20): -3.85\n",
            "Ep: 550/600, Timesteps: 638119, LR: 0.00e+00, Ep Reward: -6.10, Avg(20): -3.99\n",
            "Ep: 551/600, Timesteps: 639719, LR: 0.00e+00, Ep Reward: -3.88, Avg(20): -3.53\n",
            "Ep: 552/600, Timesteps: 641319, LR: 0.00e+00, Ep Reward: 0.05, Avg(20): -3.46\n",
            "Ep: 553/600, Timesteps: 642919, LR: 0.00e+00, Ep Reward: -5.31, Avg(20): -3.70\n",
            "Ep: 554/600, Timesteps: 644519, LR: 0.00e+00, Ep Reward: 2.69, Avg(20): -3.31\n",
            "Ep: 555/600, Timesteps: 646119, LR: 0.00e+00, Ep Reward: -4.00, Avg(20): -3.19\n",
            "Ep: 556/600, Timesteps: 647719, LR: 0.00e+00, Ep Reward: -4.75, Avg(20): -3.12\n",
            "Ep: 557/600, Timesteps: 649319, LR: 0.00e+00, Ep Reward: -5.34, Avg(20): -3.06\n",
            "Ep: 558/600, Timesteps: 650919, LR: 0.00e+00, Ep Reward: -12.46, Avg(20): -3.51\n",
            "Ep: 559/600, Timesteps: 652519, LR: 0.00e+00, Ep Reward: -5.31, Avg(20): -3.46\n",
            "Ep: 560/600, Timesteps: 654119, LR: 0.00e+00, Ep Reward: -1.81, Avg(20): -3.50\n",
            "Ep: 561/600, Timesteps: 655719, LR: 0.00e+00, Ep Reward: -1.39, Avg(20): -3.43\n",
            "Ep: 562/600, Timesteps: 657319, LR: 0.00e+00, Ep Reward: 0.49, Avg(20): -3.21\n",
            "Ep: 563/600, Timesteps: 658919, LR: 0.00e+00, Ep Reward: -14.17, Avg(20): -3.80\n",
            "Ep: 564/600, Timesteps: 660519, LR: 0.00e+00, Ep Reward: 1.10, Avg(20): -3.77\n",
            "Ep: 565/600, Timesteps: 662119, LR: 0.00e+00, Ep Reward: -18.66, Avg(20): -4.64\n",
            "Ep: 566/600, Timesteps: 663719, LR: 0.00e+00, Ep Reward: -6.34, Avg(20): -4.74\n",
            "Ep: 567/600, Timesteps: 665319, LR: 0.00e+00, Ep Reward: -10.70, Avg(20): -5.07\n",
            "Ep: 568/600, Timesteps: 666919, LR: 0.00e+00, Ep Reward: -1.18, Avg(20): -4.94\n",
            "Ep: 569/600, Timesteps: 668519, LR: 0.00e+00, Ep Reward: -5.48, Avg(20): -5.13\n",
            "Ep: 570/600, Timesteps: 670119, LR: 0.00e+00, Ep Reward: -0.51, Avg(20): -4.85\n",
            "Ep: 571/600, Timesteps: 671719, LR: 0.00e+00, Ep Reward: -1.67, Avg(20): -4.74\n",
            "Ep: 572/600, Timesteps: 673319, LR: 0.00e+00, Ep Reward: -6.40, Avg(20): -5.06\n",
            "Ep: 573/600, Timesteps: 674919, LR: 0.00e+00, Ep Reward: -0.21, Avg(20): -4.80\n",
            "Ep: 574/600, Timesteps: 676519, LR: 0.00e+00, Ep Reward: -6.79, Avg(20): -5.28\n",
            "Ep: 575/600, Timesteps: 678119, LR: 0.00e+00, Ep Reward: -6.33, Avg(20): -5.40\n",
            "Ep: 576/600, Timesteps: 679719, LR: 0.00e+00, Ep Reward: -2.52, Avg(20): -5.28\n",
            "Ep: 577/600, Timesteps: 681319, LR: 0.00e+00, Ep Reward: -3.96, Avg(20): -5.22\n",
            "Ep: 578/600, Timesteps: 682919, LR: 0.00e+00, Ep Reward: -0.21, Avg(20): -4.60\n",
            "Ep: 579/600, Timesteps: 682998, LR: 0.00e+00, Ep Reward: -115.96, Avg(20): -10.13\n",
            "Ep: 580/600, Timesteps: 684598, LR: 0.00e+00, Ep Reward: -2.74, Avg(20): -10.18\n",
            "Ep: 581/600, Timesteps: 686198, LR: 0.00e+00, Ep Reward: -9.84, Avg(20): -10.60\n",
            "Ep: 582/600, Timesteps: 687798, LR: 0.00e+00, Ep Reward: -2.65, Avg(20): -10.76\n",
            "Ep: 583/600, Timesteps: 689398, LR: 0.00e+00, Ep Reward: -6.54, Avg(20): -10.38\n",
            "Ep: 584/600, Timesteps: 690998, LR: 0.00e+00, Ep Reward: -2.20, Avg(20): -10.55\n",
            "Ep: 585/600, Timesteps: 692598, LR: 0.00e+00, Ep Reward: -0.50, Avg(20): -9.64\n",
            "Ep: 586/600, Timesteps: 694198, LR: 0.00e+00, Ep Reward: -3.40, Avg(20): -9.49\n",
            "Ep: 587/600, Timesteps: 695798, LR: 0.00e+00, Ep Reward: -2.86, Avg(20): -9.10\n",
            "Ep: 588/600, Timesteps: 697398, LR: 0.00e+00, Ep Reward: -4.22, Avg(20): -9.25\n",
            "Ep: 589/600, Timesteps: 698998, LR: 0.00e+00, Ep Reward: -6.30, Avg(20): -9.29\n",
            "\n",
            "--- Checkpoint Model Saved: ppo_models/ppo_ac_BipedalWalker-v3_ckpt_700000.pth at Timestep 700000 ---\n",
            "Ep: 590/600, Timesteps: 700598, LR: 0.00e+00, Ep Reward: -17.22, Avg(20): -10.13\n",
            "Ep: 591/600, Timesteps: 702198, LR: 0.00e+00, Ep Reward: -0.66, Avg(20): -10.08\n",
            "Ep: 592/600, Timesteps: 703798, LR: 0.00e+00, Ep Reward: -8.32, Avg(20): -10.17\n",
            "Ep: 593/600, Timesteps: 705398, LR: 0.00e+00, Ep Reward: -6.15, Avg(20): -10.47\n",
            "Ep: 594/600, Timesteps: 706998, LR: 0.00e+00, Ep Reward: -1.05, Avg(20): -10.18\n",
            "Ep: 595/600, Timesteps: 708598, LR: 0.00e+00, Ep Reward: -5.92, Avg(20): -10.16\n",
            "Ep: 596/600, Timesteps: 710198, LR: 0.00e+00, Ep Reward: 2.21, Avg(20): -9.92\n",
            "Ep: 597/600, Timesteps: 711798, LR: 0.00e+00, Ep Reward: -4.63, Avg(20): -9.96\n",
            "Ep: 598/600, Timesteps: 713398, LR: 0.00e+00, Ep Reward: -3.40, Avg(20): -10.12\n",
            "Ep: 599/600, Timesteps: 714998, LR: 0.00e+00, Ep Reward: -2.11, Avg(20): -4.42\n",
            "Ep: 600/600, Timesteps: 716598, LR: 0.00e+00, Ep Reward: -7.51, Avg(20): -4.66\n",
            "\n",
            "--- Training Finished ---\n",
            "Reason: Reached maximum episodes (600)\n",
            "Total Episodes: 600\n",
            "Total Timesteps: 716598\n",
            "Total Time: 0:38:35\n",
            "--- Final Model Saved ---\n",
            "ActorCritic Path: ppo_models/ppo_ac_BipedalWalker-v3_final_ep600_t716598.pth\n",
            "\n",
            "--- Running Final Evaluation ---\n",
            "\n",
            "--- Evaluating (10 episodes) ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-e89e822356da>:329: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=use_mixed_precision):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Eval Episode 1/10 | Reward: 19.67 | Steps: 1600\n",
            "  Eval Episode 2/10 | Reward: 10.88 | Steps: 1600\n",
            "  Eval Episode 3/10 | Reward: 9.87 | Steps: 1600\n",
            "  Eval Episode 4/10 | Reward: 6.22 | Steps: 1600\n",
            "  Eval Episode 5/10 | Reward: 7.83 | Steps: 1600\n",
            "  Eval Episode 6/10 | Reward: 3.93 | Steps: 1600\n",
            "  Eval Episode 7/10 | Reward: 11.41 | Steps: 1600\n",
            "  Eval Episode 8/10 | Reward: 14.06 | Steps: 1600\n",
            "  Eval Episode 9/10 | Reward: 13.68 | Steps: 1600\n",
            "  Eval Episode 10/10 | Reward: 13.54 | Steps: 1600\n",
            "--- Evaluation Finished | Average Reward: 11.11 ---\n",
            "Final Average Evaluation Reward (over 10 episodes): 11.11\n",
            "\n",
            "Script finished.\n"
          ]
        }
      ]
    }
  ]
}