{
  "cells": [
    {
      "cell_type": "code",
      "id": "XHn3HKUnFKpTZeRdYu9QLvhF",
      "metadata": {
        "tags": [],
        "id": "XHn3HKUnFKpTZeRdYu9QLvhF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b26e7d5b-6051-4b26-9586-91b85a902653"
      },
      "source": [
        "!sudo apt-get update\n",
        "!sudo apt-get install -y swig g++ python3-dev\n",
        "!pip install gymnasium[box2d] torch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://packages.cloud.google.com/apt gcsfuse-jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.82)] [\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.82)] [\r                                                                               \rHit:3 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: https://packages.cloud.google.com/apt/dists/gcsfuse-jammy/InRelease: Key is stored in legacy trusted.gpg keyring (/etc/apt/trusted.gpg), see the DEPRECATION section in apt-key(8) for details.\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "g++ is already the newest version (4:11.2.0-1ubuntu1).\n",
            "swig is already the newest version (4.0.2-1ubuntu1).\n",
            "python3-dev is already the newest version (3.10.6-1~22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 26 not upgraded.\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (1.1.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: box2d-py==2.3.5 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.3.5)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "import random\n",
        "from collections import deque\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "\n",
        "ENV_ID = \"BipedalWalker-v3\"\n",
        "RANDOM_SEED = 0\n",
        "ACTOR_LR = 3e-4\n",
        "CRITIC_LR = 3e-4\n",
        "ALPHA_LR = 3e-4\n",
        "DISCOUNT_FACTOR = 0.99\n",
        "TARGET_UPDATE_RATE = 0.005\n",
        "INITIAL_ALPHA = 0.2\n",
        "AUTO_TUNE_TEMP = True\n",
        "BUFFER_CAPACITY = int(1e6)\n",
        "TRAIN_BATCH_SIZE = 256\n",
        "NETWORK_HIDDEN_UNITS = 256\n",
        "EXPLORATION_STEPS = 10000\n",
        "TOTAL_TRAINING_STEPS = int(1e6)\n",
        "RENDER_EVALUATION = True\n",
        "LOGGING_INTERVAL = 1000\n",
        "REWARD_LOG_WINDOW = 100\n",
        "\n",
        "STOP_WINDOW = 20\n",
        "STOP_REWARD_TARGET = 250.0\n",
        "\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "\n",
        "compute_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {compute_device}\")\n",
        "\n",
        "\n",
        "class ExperienceReplayBuffer:\n",
        "    def __init__(self, max_size):\n",
        "        self.storage = deque(maxlen=max_size)\n",
        "        self._max_size = max_size\n",
        "\n",
        "    def add_transition(self, obs, act, rew, next_obs, terminated):\n",
        "        obs = np.expand_dims(obs, 0)\n",
        "        next_obs = np.expand_dims(next_obs, 0)\n",
        "        self.storage.append((obs, act, rew, next_obs, terminated))\n",
        "\n",
        "    def sample_batch(self, num_samples):\n",
        "        obs_batch, act_batch, rew_batch, next_obs_batch, term_batch = zip(*random.sample(self.storage, num_samples))\n",
        "        return np.concatenate(obs_batch), act_batch, rew_batch, np.concatenate(next_obs_batch), term_batch\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.storage)\n",
        "\n",
        "\n",
        "MAX_LOG_STD = 2\n",
        "MIN_LOG_STD = -20\n",
        "NUMERICAL_STABILITY_EPS = 1e-6\n",
        "\n",
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, observation_dim, action_dim, hidden_units, action_limit_high):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.layer1 = nn.Linear(observation_dim, hidden_units)\n",
        "        self.layer2 = nn.Linear(hidden_units, hidden_units)\n",
        "        self.mean_layer = nn.Linear(hidden_units, action_dim)\n",
        "        self.log_std_layer = nn.Linear(hidden_units, action_dim)\n",
        "        self.action_scale = torch.tensor(action_limit_high, dtype=torch.float32, device=compute_device)\n",
        "\n",
        "    def forward(self, observation):\n",
        "        features = F.relu(self.layer1(observation))\n",
        "        features = F.relu(self.layer2(features))\n",
        "        action_mean = self.mean_layer(features)\n",
        "        action_log_std = self.log_std_layer(features)\n",
        "        action_log_std = torch.clamp(action_log_std, min=MIN_LOG_STD, max=MAX_LOG_STD)\n",
        "        return action_mean, action_log_std\n",
        "\n",
        "    def sample_action(self, observation):\n",
        "        mean, log_std = self.forward(observation)\n",
        "        std = log_std.exp()\n",
        "        distribution = Normal(mean, std)\n",
        "        raw_action = distribution.rsample()\n",
        "        squashed_action = torch.tanh(raw_action)\n",
        "        scaled_action = squashed_action * self.action_scale\n",
        "        log_prob = distribution.log_prob(raw_action)\n",
        "\n",
        "        log_prob -= torch.log(self.action_scale * (1 - squashed_action.pow(2)) + NUMERICAL_STABILITY_EPS)\n",
        "        log_prob = log_prob.sum(1, keepdim=True)\n",
        "        deterministic_action = torch.tanh(mean) * self.action_scale\n",
        "        return scaled_action, log_prob, deterministic_action\n",
        "\n",
        "class QValueNetwork(nn.Module):\n",
        "    def __init__(self, observation_dim, action_dim, hidden_units):\n",
        "        super(QValueNetwork, self).__init__()\n",
        "\n",
        "        self.q1_layer1 = nn.Linear(observation_dim + action_dim, hidden_units)\n",
        "        self.q1_layer2 = nn.Linear(hidden_units, hidden_units)\n",
        "        self.q1_output = nn.Linear(hidden_units, 1)\n",
        "\n",
        "        self.q2_layer1 = nn.Linear(observation_dim + action_dim, hidden_units)\n",
        "        self.q2_layer2 = nn.Linear(hidden_units, hidden_units)\n",
        "        self.q2_output = nn.Linear(hidden_units, 1)\n",
        "\n",
        "    def forward(self, observation, action):\n",
        "        state_action_input = torch.cat([observation, action], 1)\n",
        "\n",
        "        q1_val = F.relu(self.q1_layer1(state_action_input))\n",
        "        q1_val = F.relu(self.q1_layer2(q1_val))\n",
        "        q1_val = self.q1_output(q1_val)\n",
        "\n",
        "        q2_val = F.relu(self.q2_layer1(state_action_input))\n",
        "        q2_val = F.relu(self.q2_layer2(q2_val))\n",
        "        q2_val = self.q2_output(q2_val)\n",
        "        return q1_val, q2_val\n",
        "\n",
        "\n",
        "class SoftActorCriticAgent:\n",
        "    def __init__(self, obs_dim, act_dim, act_limit, hidden_dim, actor_lr, critic_lr, alpha_lr, gamma, tau, alpha_init, tune_alpha, target_ent=None):\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.temperature = alpha_init\n",
        "        self.tune_temperature = tune_alpha\n",
        "\n",
        "\n",
        "        self.actor_net = PolicyNetwork(obs_dim, act_dim, hidden_dim, act_limit).to(compute_device)\n",
        "        self.actor_optim = optim.Adam(self.actor_net.parameters(), lr=actor_lr)\n",
        "\n",
        "\n",
        "        self.critic_net = QValueNetwork(obs_dim, act_dim, hidden_dim).to(compute_device)\n",
        "        self.critic_target_net = QValueNetwork(obs_dim, act_dim, hidden_dim).to(compute_device)\n",
        "        self.critic_target_net.load_state_dict(self.critic_net.state_dict())\n",
        "        self.critic_optim = optim.Adam(self.critic_net.parameters(), lr=critic_lr)\n",
        "\n",
        "\n",
        "        if self.tune_temperature:\n",
        "            if target_ent is None:\n",
        "                self.target_entropy = -float(act_dim)\n",
        "            else:\n",
        "                self.target_entropy = target_ent\n",
        "            self.log_temperature = torch.zeros(1, requires_grad=True, device=compute_device)\n",
        "            self.temperature_optim = optim.Adam([self.log_temperature], lr=alpha_lr)\n",
        "            self.temperature = self.log_temperature.exp().item()\n",
        "        else:\n",
        "            self.target_entropy = None\n",
        "            self.log_temperature = None\n",
        "            self.temperature_optim = None\n",
        "\n",
        "    def get_action(self, current_obs, is_eval=False):\n",
        "        current_obs_tensor = torch.FloatTensor(current_obs).to(compute_device).unsqueeze(0)\n",
        "        if is_eval is False:\n",
        "            action_tensor, _, _ = self.actor_net.sample_action(current_obs_tensor)\n",
        "        else:\n",
        "            _, _, action_tensor = self.actor_net.sample_action(current_obs_tensor)\n",
        "        return action_tensor.detach().cpu().numpy()[0]\n",
        "\n",
        "    def train_step(self, replay_buffer, sample_size):\n",
        "        if len(replay_buffer) < sample_size:\n",
        "            return 0.0, 0.0, 0.0\n",
        "\n",
        "\n",
        "        obs_sample, act_sample, rew_sample, next_obs_sample, term_sample = replay_buffer.sample_batch(sample_size)\n",
        "\n",
        "\n",
        "        obs_tensor = torch.FloatTensor(obs_sample).to(compute_device)\n",
        "        next_obs_tensor = torch.FloatTensor(next_obs_sample).to(compute_device)\n",
        "        act_tensor = torch.FloatTensor(np.array(act_sample)).to(compute_device)\n",
        "        rew_tensor = torch.FloatTensor(rew_sample).to(compute_device).unsqueeze(1)\n",
        "        term_tensor = torch.FloatTensor(np.float32(term_sample)).to(compute_device).unsqueeze(1)\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            next_act_tensor, next_log_prob_tensor, _ = self.actor_net.sample_action(next_obs_tensor)\n",
        "            q1_target_next, q2_target_next = self.critic_target_net(next_obs_tensor, next_act_tensor)\n",
        "            min_q_target_next = torch.min(q1_target_next, q2_target_next)\n",
        "\n",
        "            q_target = rew_tensor + (1 - term_tensor) * self.gamma * (min_q_target_next - self.temperature * next_log_prob_tensor)\n",
        "\n",
        "        current_q1, current_q2 = self.critic_net(obs_tensor, act_tensor)\n",
        "        q1_loss = F.mse_loss(current_q1, q_target)\n",
        "        q2_loss = F.mse_loss(current_q2, q_target)\n",
        "        critic_total_loss = q1_loss + q2_loss\n",
        "\n",
        "        self.critic_optim.zero_grad()\n",
        "        critic_total_loss.backward()\n",
        "        self.critic_optim.step()\n",
        "\n",
        "\n",
        "        for p in self.critic_net.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        sampled_actions, log_probs, _ = self.actor_net.sample_action(obs_tensor)\n",
        "        q1_for_actor, q2_for_actor = self.critic_net(obs_tensor, sampled_actions)\n",
        "        min_q_for_actor = torch.min(q1_for_actor, q2_for_actor)\n",
        "\n",
        "        actor_policy_loss = (self.temperature * log_probs - min_q_for_actor).mean()\n",
        "\n",
        "        self.actor_optim.zero_grad()\n",
        "        actor_policy_loss.backward()\n",
        "        self.actor_optim.step()\n",
        "\n",
        "\n",
        "        for p in self.critic_net.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "\n",
        "        temp_loss = torch.tensor(0.).to(compute_device)\n",
        "        if self.tune_temperature:\n",
        "\n",
        "            temp_loss = -(self.log_temperature.exp() * (log_probs + self.target_entropy).detach()).mean()\n",
        "\n",
        "            self.temperature_optim.zero_grad()\n",
        "            temp_loss.backward()\n",
        "            self.temperature_optim.step()\n",
        "            self.temperature = self.log_temperature.exp().item()\n",
        "\n",
        "\n",
        "        for target_p, local_p in zip(self.critic_target_net.parameters(), self.critic_net.parameters()):\n",
        "            target_p.data.copy_(self.tau * local_p.data + (1.0 - self.tau) * target_p.data)\n",
        "\n",
        "        return critic_total_loss.item(), actor_policy_loss.item(), temp_loss.item()\n",
        "\n",
        "\n",
        "def run_evaluation(agent_policy, environment, num_episodes=10, should_render=False):\n",
        "    total_rew = 0.0\n",
        "    print(f\"\\n--- Starting Final Evaluation ({num_episodes} episodes) ---\")\n",
        "    for ep_idx in range(num_episodes):\n",
        "        obs, _ = environment.reset()\n",
        "        ep_reward = 0.0\n",
        "        terminated = False\n",
        "        truncated = False\n",
        "        t = 0\n",
        "        while not terminated and not truncated:\n",
        "            action = agent_policy.get_action(obs, is_eval=True)\n",
        "            next_obs, reward, terminated, truncated, _ = environment.step(action)\n",
        "            ep_reward += reward\n",
        "            obs = next_obs\n",
        "            t += 1\n",
        "            if should_render:\n",
        "                try:\n",
        "                    environment.render()\n",
        "                    time.sleep(0.01)\n",
        "                except Exception as render_err:\n",
        "                    print(f\"Rendering failed during eval ep {ep_idx+1}: {render_err}\")\n",
        "                    should_render = False\n",
        "        print(f\"  Eval Episode {ep_idx+1}/{num_episodes} | Reward: {ep_reward:.2f}\")\n",
        "        total_rew += ep_reward\n",
        "    avg_rew = total_rew / num_episodes\n",
        "    print(f\"--- Final Evaluation Finished | Average Reward: {avg_rew:.2f} ---\")\n",
        "    return avg_rew\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_env = gym.make(ENV_ID, render_mode=None)\n",
        "    eval_render_mode = \"human\" if RENDER_EVALUATION else None\n",
        "    eval_environment = gym.make(ENV_ID, render_mode=eval_render_mode)\n",
        "\n",
        "    obs_dimension = train_env.observation_space.shape[0]\n",
        "    act_dimension = train_env.action_space.shape[0]\n",
        "    action_limit = train_env.action_space.high[0] if isinstance(train_env.action_space.high, np.ndarray) else train_env.action_space.high\n",
        "\n",
        "    print(f\"Environment: {ENV_ID}\")\n",
        "    print(f\"State Dim: {obs_dimension}, Action Dim: {act_dimension}, Action Max: {action_limit}\")\n",
        "    print(f\"Max Timesteps: {TOTAL_TRAINING_STEPS}\")\n",
        "    print(f\"Early Stopping: Avg Reward > {STOP_REWARD_TARGET} over last {STOP_WINDOW} episodes.\")\n",
        "\n",
        "    sac_agent = SoftActorCriticAgent(obs_dimension, act_dimension, action_limit, NETWORK_HIDDEN_UNITS, ACTOR_LR, CRITIC_LR, ALPHA_LR, DISCOUNT_FACTOR, TARGET_UPDATE_RATE, INITIAL_ALPHA, AUTO_TUNE_TEMP)\n",
        "\n",
        "    replay_memory = ExperienceReplayBuffer(BUFFER_CAPACITY)\n",
        "\n",
        "    total_steps = 0\n",
        "    ep_count = 0\n",
        "    current_ep_reward = 0\n",
        "    current_ep_steps = 0\n",
        "    is_done = False\n",
        "    is_truncated = False\n",
        "    stopped_early = False\n",
        "    current_obs, _ = train_env.reset(seed=RANDOM_SEED)\n",
        "\n",
        "    training_start_time = time.time()\n",
        "    reward_log_deque = deque(maxlen=REWARD_LOG_WINDOW)\n",
        "    stopping_deque = deque(maxlen=STOP_WINDOW)\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "\n",
        "    while total_steps < TOTAL_TRAINING_STEPS and not stopped_early:\n",
        "        current_ep_steps += 1\n",
        "        total_steps += 1\n",
        "\n",
        "        if total_steps < EXPLORATION_STEPS:\n",
        "            current_action = train_env.action_space.sample()\n",
        "        else:\n",
        "            current_action = sac_agent.get_action(current_obs)\n",
        "\n",
        "        next_obs, reward_val, is_terminated, is_truncated, _ = train_env.step(current_action)\n",
        "        is_done = is_terminated or is_truncated\n",
        "        done_signal = float(is_done)\n",
        "\n",
        "        replay_memory.add_transition(current_obs, current_action, reward_val, next_obs, done_signal)\n",
        "\n",
        "        current_obs = next_obs\n",
        "        current_ep_reward += reward_val\n",
        "\n",
        "        if total_steps > EXPLORATION_STEPS:\n",
        "            crit_loss, act_loss, alph_loss = sac_agent.train_step(replay_memory, TRAIN_BATCH_SIZE)\n",
        "\n",
        "            if total_steps % (LOGGING_INTERVAL * 5) == 0:\n",
        "                 print(f\"       T: {total_steps}/{TOTAL_TRAINING_STEPS} | Losses C:{crit_loss:.2f} A:{act_loss:.2f} Alpha:{alph_loss:.2f} | Alpha: {sac_agent.temperature:.3f}\")\n",
        "\n",
        "        if is_done:\n",
        "            reward_log_deque.append(current_ep_reward)\n",
        "            stopping_deque.append(current_ep_reward)\n",
        "\n",
        "            avg_reward_log = np.mean(reward_log_deque) if reward_log_deque else 0.0\n",
        "\n",
        "            elapsed_seconds = time.time() - training_start_time\n",
        "            time_str = str(datetime.timedelta(seconds=int(elapsed_seconds)))\n",
        "\n",
        "            print(f\"Ep {ep_count+1}, Reward: {current_ep_reward:.2f}, Avg ({REWARD_LOG_WINDOW}): {avg_reward_log:.2f}, Steps: {current_ep_steps}, Total T: {total_steps}/{TOTAL_TRAINING_STEPS}, Time: {time_str}\")\n",
        "\n",
        "\n",
        "            if len(stopping_deque) == STOP_WINDOW:\n",
        "                avg_reward_stopping = np.mean(stopping_deque)\n",
        "                print(f\"  Avg Reward (last {STOP_WINDOW} eps): {avg_reward_stopping:.2f} (Threshold: {STOP_REWARD_TARGET})\")\n",
        "                if avg_reward_stopping > STOP_REWARD_TARGET:\n",
        "                    print(f\"\\n--- Early Stopping Condition Met! ---\")\n",
        "                    print(f\"Average reward over last {STOP_WINDOW} episodes ({avg_reward_stopping:.2f}) exceeded threshold ({STOP_REWARD_TARGET}).\")\n",
        "                    stopped_early = True\n",
        "\n",
        "\n",
        "            current_obs, _ = train_env.reset()\n",
        "            current_ep_reward = 0\n",
        "            current_ep_steps = 0\n",
        "            ep_count += 1\n",
        "            is_done = False\n",
        "            is_truncated = False\n",
        "\n",
        "\n",
        "    elapsed_seconds = time.time() - training_start_time\n",
        "    time_str = str(datetime.timedelta(seconds=int(elapsed_seconds)))\n",
        "    stop_msg = 'Early Stop' if stopped_early else 'Max Timesteps'\n",
        "    print(f\"\\n--- Training Finished ({stop_msg}) ---\")\n",
        "    print(f\"Total Timesteps: {total_steps}\")\n",
        "    print(f\"Total Episodes: {ep_count}\")\n",
        "    print(f\"Total Time: {time_str}\")\n",
        "\n",
        "\n",
        "    final_eval_reward = run_evaluation(sac_agent, eval_environment, num_episodes=10, should_render=RENDER_EVALUATION)\n",
        "\n",
        "\n",
        "    stop_tag = \"earlystop\" if stopped_early else \"maxt\"\n",
        "    actor_save_path = f'sac_actor_{ENV_ID}_{stop_tag}_{total_steps}.pth'\n",
        "    critic_save_path = f'sac_critic_{ENV_ID}_{stop_tag}_{total_steps}.pth'\n",
        "    torch.save(sac_agent.actor_net.state_dict(), actor_save_path)\n",
        "    torch.save(sac_agent.critic_net.state_dict(), critic_save_path)\n",
        "    print(f\"--- Final Model Saved ---\")\n",
        "    print(f\"Actor: {actor_save_path}\")\n",
        "    print(f\"Critic: {critic_save_path}\")\n",
        "\n",
        "\n",
        "    train_env.close()\n",
        "    eval_environment.close()\n",
        "    print(\"Script finished.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YIOnswZIg5l5",
        "outputId": "314870a1-17e5-4aa5-c83a-8a86c53bf759"
      },
      "id": "YIOnswZIg5l5",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Environment: BipedalWalker-v3\n",
            "State Dim: 24, Action Dim: 4, Action Max: 1.0\n",
            "Max Timesteps: 1000000\n",
            "Early Stopping: Avg Reward > 250.0 over last 20 episodes.\n",
            "Starting training...\n",
            "Ep 1, Reward: -108.19, Avg (100): -108.19, Steps: 51, Total T: 51/1000000, Time: 0:00:00\n",
            "Ep 2, Reward: -114.23, Avg (100): -111.21, Steps: 96, Total T: 147/1000000, Time: 0:00:00\n",
            "Ep 3, Reward: -80.16, Avg (100): -100.86, Steps: 1600, Total T: 1747/1000000, Time: 0:00:01\n",
            "Ep 4, Reward: -109.20, Avg (100): -102.94, Steps: 88, Total T: 1835/1000000, Time: 0:00:01\n",
            "Ep 5, Reward: -120.91, Avg (100): -106.54, Steps: 124, Total T: 1959/1000000, Time: 0:00:01\n",
            "Ep 6, Reward: -117.94, Avg (100): -108.44, Steps: 91, Total T: 2050/1000000, Time: 0:00:01\n",
            "Ep 7, Reward: -119.60, Avg (100): -110.03, Steps: 79, Total T: 2129/1000000, Time: 0:00:01\n",
            "Ep 8, Reward: -81.26, Avg (100): -106.44, Steps: 1600, Total T: 3729/1000000, Time: 0:00:02\n",
            "Ep 9, Reward: -113.52, Avg (100): -107.22, Steps: 49, Total T: 3778/1000000, Time: 0:00:02\n",
            "Ep 10, Reward: -100.90, Avg (100): -106.59, Steps: 67, Total T: 3845/1000000, Time: 0:00:02\n",
            "Ep 11, Reward: -97.40, Avg (100): -105.76, Steps: 80, Total T: 3925/1000000, Time: 0:00:02\n",
            "Ep 12, Reward: -81.85, Avg (100): -103.76, Steps: 1600, Total T: 5525/1000000, Time: 0:00:03\n",
            "Ep 13, Reward: -100.77, Avg (100): -103.53, Steps: 63, Total T: 5588/1000000, Time: 0:00:03\n",
            "Ep 14, Reward: -117.68, Avg (100): -104.54, Steps: 87, Total T: 5675/1000000, Time: 0:00:03\n",
            "Ep 15, Reward: -110.57, Avg (100): -104.94, Steps: 58, Total T: 5733/1000000, Time: 0:00:03\n",
            "Ep 16, Reward: -108.87, Avg (100): -105.19, Steps: 90, Total T: 5823/1000000, Time: 0:00:03\n",
            "Ep 17, Reward: -91.83, Avg (100): -104.40, Steps: 1600, Total T: 7423/1000000, Time: 0:00:04\n",
            "Ep 18, Reward: -100.25, Avg (100): -104.17, Steps: 72, Total T: 7495/1000000, Time: 0:00:04\n",
            "Ep 19, Reward: -98.27, Avg (100): -103.86, Steps: 88, Total T: 7583/1000000, Time: 0:00:04\n",
            "Ep 20, Reward: -97.92, Avg (100): -103.57, Steps: 88, Total T: 7671/1000000, Time: 0:00:04\n",
            "  Avg Reward (last 20 eps): -103.57 (Threshold: 250.0)\n",
            "Ep 21, Reward: -102.52, Avg (100): -103.52, Steps: 67, Total T: 7738/1000000, Time: 0:00:04\n",
            "  Avg Reward (last 20 eps): -103.28 (Threshold: 250.0)\n",
            "Ep 22, Reward: -101.13, Avg (100): -103.41, Steps: 78, Total T: 7816/1000000, Time: 0:00:04\n",
            "  Avg Reward (last 20 eps): -102.63 (Threshold: 250.0)\n",
            "Ep 23, Reward: -103.96, Avg (100): -103.43, Steps: 70, Total T: 7886/1000000, Time: 0:00:04\n",
            "  Avg Reward (last 20 eps): -103.82 (Threshold: 250.0)\n",
            "Ep 24, Reward: -105.82, Avg (100): -103.53, Steps: 33, Total T: 7919/1000000, Time: 0:00:04\n",
            "  Avg Reward (last 20 eps): -103.65 (Threshold: 250.0)\n",
            "Ep 25, Reward: -78.39, Avg (100): -102.53, Steps: 1600, Total T: 9519/1000000, Time: 0:00:05\n",
            "  Avg Reward (last 20 eps): -101.52 (Threshold: 250.0)\n",
            "Ep 26, Reward: -106.99, Avg (100): -102.70, Steps: 109, Total T: 9628/1000000, Time: 0:00:05\n",
            "  Avg Reward (last 20 eps): -100.98 (Threshold: 250.0)\n",
            "Ep 27, Reward: -86.24, Avg (100): -102.09, Steps: 1600, Total T: 11228/1000000, Time: 0:00:41\n",
            "  Avg Reward (last 20 eps): -99.31 (Threshold: 250.0)\n",
            "Ep 28, Reward: -114.59, Avg (100): -102.53, Steps: 102, Total T: 11330/1000000, Time: 0:00:43\n",
            "  Avg Reward (last 20 eps): -100.97 (Threshold: 250.0)\n",
            "Ep 29, Reward: -115.36, Avg (100): -102.98, Steps: 80, Total T: 11410/1000000, Time: 0:00:46\n",
            "  Avg Reward (last 20 eps): -101.07 (Threshold: 250.0)\n",
            "Ep 30, Reward: -124.26, Avg (100): -103.69, Steps: 124, Total T: 11534/1000000, Time: 0:00:49\n",
            "  Avg Reward (last 20 eps): -102.23 (Threshold: 250.0)\n",
            "Ep 31, Reward: -117.48, Avg (100): -104.13, Steps: 80, Total T: 11614/1000000, Time: 0:00:52\n",
            "  Avg Reward (last 20 eps): -103.24 (Threshold: 250.0)\n",
            "Ep 32, Reward: -111.95, Avg (100): -104.38, Steps: 102, Total T: 11716/1000000, Time: 0:00:55\n",
            "  Avg Reward (last 20 eps): -104.74 (Threshold: 250.0)\n",
            "Ep 33, Reward: -101.78, Avg (100): -104.30, Steps: 67, Total T: 11783/1000000, Time: 0:00:56\n",
            "  Avg Reward (last 20 eps): -104.79 (Threshold: 250.0)\n",
            "Ep 34, Reward: -111.96, Avg (100): -104.52, Steps: 74, Total T: 11857/1000000, Time: 0:00:58\n",
            "  Avg Reward (last 20 eps): -104.51 (Threshold: 250.0)\n",
            "Ep 35, Reward: -93.17, Avg (100): -104.20, Steps: 1600, Total T: 13457/1000000, Time: 0:01:42\n",
            "  Avg Reward (last 20 eps): -103.64 (Threshold: 250.0)\n",
            "Ep 36, Reward: -116.86, Avg (100): -104.55, Steps: 88, Total T: 13545/1000000, Time: 0:01:44\n",
            "  Avg Reward (last 20 eps): -104.04 (Threshold: 250.0)\n",
            "Ep 37, Reward: -128.63, Avg (100): -105.20, Steps: 125, Total T: 13670/1000000, Time: 0:01:48\n",
            "  Avg Reward (last 20 eps): -105.88 (Threshold: 250.0)\n",
            "Ep 38, Reward: -99.89, Avg (100): -105.06, Steps: 80, Total T: 13750/1000000, Time: 0:01:50\n",
            "  Avg Reward (last 20 eps): -105.86 (Threshold: 250.0)\n",
            "      T: 15000/1000000 | Losses C:0.87 A:-27.26 Alpha:1.85 | Alpha: 0.294\n",
            "Ep 39, Reward: -73.33, Avg (100): -104.25, Steps: 1600, Total T: 15350/1000000, Time: 0:02:37\n",
            "  Avg Reward (last 20 eps): -104.61 (Threshold: 250.0)\n",
            "Ep 40, Reward: -103.67, Avg (100): -104.23, Steps: 1600, Total T: 16950/1000000, Time: 0:03:25\n",
            "  Avg Reward (last 20 eps): -104.90 (Threshold: 250.0)\n",
            "Ep 41, Reward: -104.76, Avg (100): -104.25, Steps: 63, Total T: 17013/1000000, Time: 0:03:27\n",
            "  Avg Reward (last 20 eps): -105.01 (Threshold: 250.0)\n",
            "Ep 42, Reward: -73.27, Avg (100): -103.51, Steps: 1600, Total T: 18613/1000000, Time: 0:04:11\n",
            "  Avg Reward (last 20 eps): -103.62 (Threshold: 250.0)\n",
            "      T: 20000/1000000 | Losses C:7.71 A:-27.21 Alpha:0.57 | Alpha: 0.098\n",
            "Ep 43, Reward: -80.16, Avg (100): -102.97, Steps: 1600, Total T: 20213/1000000, Time: 0:04:56\n",
            "  Avg Reward (last 20 eps): -102.43 (Threshold: 250.0)\n",
            "Ep 44, Reward: -79.36, Avg (100): -102.43, Steps: 1600, Total T: 21813/1000000, Time: 0:05:39\n",
            "  Avg Reward (last 20 eps): -101.11 (Threshold: 250.0)\n",
            "Ep 45, Reward: -79.19, Avg (100): -101.91, Steps: 1600, Total T: 23413/1000000, Time: 0:06:22\n",
            "  Avg Reward (last 20 eps): -101.15 (Threshold: 250.0)\n",
            "      T: 25000/1000000 | Losses C:4.98 A:-21.79 Alpha:0.16 | Alpha: 0.033\n",
            "Ep 46, Reward: -83.14, Avg (100): -101.50, Steps: 1600, Total T: 25013/1000000, Time: 0:07:06\n",
            "  Avg Reward (last 20 eps): -99.95 (Threshold: 250.0)\n",
            "Ep 47, Reward: -78.99, Avg (100): -101.03, Steps: 1600, Total T: 26613/1000000, Time: 0:07:50\n",
            "  Avg Reward (last 20 eps): -99.59 (Threshold: 250.0)\n",
            "Ep 48, Reward: -136.10, Avg (100): -101.76, Steps: 1285, Total T: 27898/1000000, Time: 0:08:25\n",
            "  Avg Reward (last 20 eps): -100.67 (Threshold: 250.0)\n",
            "Ep 49, Reward: -44.31, Avg (100): -100.58, Steps: 1600, Total T: 29498/1000000, Time: 0:09:11\n",
            "  Avg Reward (last 20 eps): -97.11 (Threshold: 250.0)\n",
            "      T: 30000/1000000 | Losses C:0.19 A:-17.62 Alpha:0.03 | Alpha: 0.012\n",
            "Ep 50, Reward: -48.41, Avg (100): -99.54, Steps: 1600, Total T: 31098/1000000, Time: 0:09:56\n",
            "  Avg Reward (last 20 eps): -93.32 (Threshold: 250.0)\n",
            "Ep 51, Reward: -41.20, Avg (100): -98.40, Steps: 1600, Total T: 32698/1000000, Time: 0:10:36\n",
            "  Avg Reward (last 20 eps): -89.51 (Threshold: 250.0)\n",
            "Ep 52, Reward: -109.51, Avg (100): -98.61, Steps: 83, Total T: 32781/1000000, Time: 0:10:39\n",
            "  Avg Reward (last 20 eps): -89.39 (Threshold: 250.0)\n",
            "Ep 53, Reward: -39.71, Avg (100): -97.50, Steps: 1600, Total T: 34381/1000000, Time: 0:11:22\n",
            "  Avg Reward (last 20 eps): -86.28 (Threshold: 250.0)\n",
            "Ep 54, Reward: -102.44, Avg (100): -97.59, Steps: 181, Total T: 34562/1000000, Time: 0:11:28\n",
            "  Avg Reward (last 20 eps): -85.81 (Threshold: 250.0)\n",
            "Ep 55, Reward: -132.52, Avg (100): -98.23, Steps: 138, Total T: 34700/1000000, Time: 0:11:32\n",
            "  Avg Reward (last 20 eps): -87.77 (Threshold: 250.0)\n",
            "Ep 56, Reward: -112.41, Avg (100): -98.48, Steps: 106, Total T: 34806/1000000, Time: 0:11:35\n",
            "  Avg Reward (last 20 eps): -87.55 (Threshold: 250.0)\n",
            "Ep 57, Reward: -110.34, Avg (100): -98.69, Steps: 132, Total T: 34938/1000000, Time: 0:11:39\n",
            "  Avg Reward (last 20 eps): -86.64 (Threshold: 250.0)\n",
            "      T: 35000/1000000 | Losses C:0.27 A:-13.18 Alpha:0.01 | Alpha: 0.005\n",
            "Ep 58, Reward: -103.00, Avg (100): -98.76, Steps: 127, Total T: 35065/1000000, Time: 0:11:43\n",
            "  Avg Reward (last 20 eps): -86.79 (Threshold: 250.0)\n",
            "Ep 59, Reward: -131.32, Avg (100): -99.31, Steps: 136, Total T: 35201/1000000, Time: 0:11:47\n",
            "  Avg Reward (last 20 eps): -89.69 (Threshold: 250.0)\n",
            "Ep 60, Reward: -96.31, Avg (100): -99.26, Steps: 1600, Total T: 36801/1000000, Time: 0:12:30\n",
            "  Avg Reward (last 20 eps): -89.32 (Threshold: 250.0)\n",
            "Ep 61, Reward: -119.55, Avg (100): -99.60, Steps: 75, Total T: 36876/1000000, Time: 0:12:32\n",
            "  Avg Reward (last 20 eps): -90.06 (Threshold: 250.0)\n",
            "Ep 62, Reward: -125.87, Avg (100): -100.02, Steps: 88, Total T: 36964/1000000, Time: 0:12:34\n",
            "  Avg Reward (last 20 eps): -92.69 (Threshold: 250.0)\n",
            "Ep 63, Reward: -59.68, Avg (100): -99.38, Steps: 1600, Total T: 38564/1000000, Time: 0:13:21\n",
            "  Avg Reward (last 20 eps): -91.67 (Threshold: 250.0)\n",
            "Ep 64, Reward: -110.76, Avg (100): -99.56, Steps: 283, Total T: 38847/1000000, Time: 0:13:29\n",
            "  Avg Reward (last 20 eps): -93.24 (Threshold: 250.0)\n",
            "Ep 65, Reward: -108.41, Avg (100): -99.69, Steps: 165, Total T: 39012/1000000, Time: 0:13:34\n",
            "  Avg Reward (last 20 eps): -94.70 (Threshold: 250.0)\n",
            "      T: 40000/1000000 | Losses C:0.24 A:-9.49 Alpha:0.00 | Alpha: 0.003\n",
            "Ep 66, Reward: -29.68, Avg (100): -98.63, Steps: 1600, Total T: 40612/1000000, Time: 0:14:19\n",
            "  Avg Reward (last 20 eps): -92.03 (Threshold: 250.0)\n",
            "Ep 67, Reward: -131.37, Avg (100): -99.12, Steps: 1421, Total T: 42033/1000000, Time: 0:14:55\n",
            "  Avg Reward (last 20 eps): -94.65 (Threshold: 250.0)\n",
            "Ep 68, Reward: -46.48, Avg (100): -98.35, Steps: 1600, Total T: 43633/1000000, Time: 0:15:42\n",
            "  Avg Reward (last 20 eps): -90.16 (Threshold: 250.0)\n",
            "      T: 45000/1000000 | Losses C:0.58 A:-6.15 Alpha:0.00 | Alpha: 0.002\n",
            "Ep 69, Reward: -65.53, Avg (100): -97.87, Steps: 1600, Total T: 45233/1000000, Time: 0:16:30\n",
            "  Avg Reward (last 20 eps): -91.23 (Threshold: 250.0)\n",
            "Ep 70, Reward: -73.65, Avg (100): -97.52, Steps: 1600, Total T: 46833/1000000, Time: 0:17:18\n",
            "  Avg Reward (last 20 eps): -92.49 (Threshold: 250.0)\n",
            "Ep 71, Reward: -78.54, Avg (100): -97.26, Steps: 1600, Total T: 48433/1000000, Time: 0:18:06\n",
            "  Avg Reward (last 20 eps): -94.35 (Threshold: 250.0)\n",
            "      T: 50000/1000000 | Losses C:1.19 A:-3.88 Alpha:0.00 | Alpha: 0.003\n",
            "Ep 72, Reward: -71.79, Avg (100): -96.90, Steps: 1600, Total T: 50033/1000000, Time: 0:18:55\n",
            "  Avg Reward (last 20 eps): -92.47 (Threshold: 250.0)\n",
            "Ep 73, Reward: -68.64, Avg (100): -96.52, Steps: 1600, Total T: 51633/1000000, Time: 0:19:43\n",
            "  Avg Reward (last 20 eps): -93.91 (Threshold: 250.0)\n",
            "Ep 74, Reward: -63.51, Avg (100): -96.07, Steps: 1600, Total T: 53233/1000000, Time: 0:20:29\n",
            "  Avg Reward (last 20 eps): -91.97 (Threshold: 250.0)\n",
            "Ep 75, Reward: -52.85, Avg (100): -95.49, Steps: 1600, Total T: 54833/1000000, Time: 0:21:16\n",
            "  Avg Reward (last 20 eps): -87.98 (Threshold: 250.0)\n",
            "      T: 55000/1000000 | Losses C:0.68 A:-3.10 Alpha:-0.00 | Alpha: 0.003\n",
            "Ep 76, Reward: -95.95, Avg (100): -95.50, Steps: 1600, Total T: 56433/1000000, Time: 0:22:03\n",
            "  Avg Reward (last 20 eps): -87.16 (Threshold: 250.0)\n",
            "Ep 77, Reward: -54.56, Avg (100): -94.97, Steps: 1600, Total T: 58033/1000000, Time: 0:22:45\n",
            "  Avg Reward (last 20 eps): -84.37 (Threshold: 250.0)\n",
            "Ep 78, Reward: -61.81, Avg (100): -94.54, Steps: 1600, Total T: 59633/1000000, Time: 0:23:26\n",
            "  Avg Reward (last 20 eps): -82.31 (Threshold: 250.0)\n",
            "      T: 60000/1000000 | Losses C:0.20 A:-1.60 Alpha:-0.00 | Alpha: 0.002\n",
            "Ep 79, Reward: -36.94, Avg (100): -93.81, Steps: 1600, Total T: 61233/1000000, Time: 0:24:10\n",
            "  Avg Reward (last 20 eps): -77.59 (Threshold: 250.0)\n",
            "Ep 80, Reward: -31.27, Avg (100): -93.03, Steps: 1600, Total T: 62833/1000000, Time: 0:24:52\n",
            "  Avg Reward (last 20 eps): -74.34 (Threshold: 250.0)\n",
            "Ep 81, Reward: -38.12, Avg (100): -92.35, Steps: 1600, Total T: 64433/1000000, Time: 0:25:37\n",
            "  Avg Reward (last 20 eps): -70.27 (Threshold: 250.0)\n",
            "      T: 65000/1000000 | Losses C:0.06 A:-2.11 Alpha:0.00 | Alpha: 0.002\n",
            "Ep 82, Reward: -39.44, Avg (100): -91.71, Steps: 1600, Total T: 66033/1000000, Time: 0:26:18\n",
            "  Avg Reward (last 20 eps): -65.95 (Threshold: 250.0)\n",
            "Ep 83, Reward: -38.95, Avg (100): -91.07, Steps: 1600, Total T: 67633/1000000, Time: 0:27:07\n",
            "  Avg Reward (last 20 eps): -64.91 (Threshold: 250.0)\n",
            "Ep 84, Reward: -31.72, Avg (100): -90.37, Steps: 1600, Total T: 69233/1000000, Time: 0:27:54\n",
            "  Avg Reward (last 20 eps): -60.96 (Threshold: 250.0)\n",
            "Ep 85, Reward: -108.37, Avg (100): -90.58, Steps: 56, Total T: 69289/1000000, Time: 0:27:56\n",
            "  Avg Reward (last 20 eps): -60.96 (Threshold: 250.0)\n",
            "      T: 70000/1000000 | Losses C:0.09 A:-0.88 Alpha:0.00 | Alpha: 0.002\n",
            "Ep 86, Reward: -142.18, Avg (100): -91.18, Steps: 1493, Total T: 70782/1000000, Time: 0:28:38\n",
            "  Avg Reward (last 20 eps): -66.58 (Threshold: 250.0)\n",
            "Ep 87, Reward: -109.93, Avg (100): -91.39, Steps: 57, Total T: 70839/1000000, Time: 0:28:39\n",
            "  Avg Reward (last 20 eps): -65.51 (Threshold: 250.0)\n",
            "Ep 88, Reward: -112.84, Avg (100): -91.64, Steps: 370, Total T: 71209/1000000, Time: 0:28:50\n",
            "  Avg Reward (last 20 eps): -68.83 (Threshold: 250.0)\n",
            "Ep 89, Reward: -156.55, Avg (100): -92.37, Steps: 1126, Total T: 72335/1000000, Time: 0:29:20\n",
            "  Avg Reward (last 20 eps): -73.38 (Threshold: 250.0)\n",
            "Ep 90, Reward: -110.54, Avg (100): -92.57, Steps: 60, Total T: 72395/1000000, Time: 0:29:22\n",
            "  Avg Reward (last 20 eps): -75.23 (Threshold: 250.0)\n",
            "Ep 91, Reward: -111.74, Avg (100): -92.78, Steps: 55, Total T: 72450/1000000, Time: 0:29:24\n",
            "  Avg Reward (last 20 eps): -76.89 (Threshold: 250.0)\n",
            "Ep 92, Reward: -100.56, Avg (100): -92.86, Steps: 112, Total T: 72562/1000000, Time: 0:29:27\n",
            "  Avg Reward (last 20 eps): -78.32 (Threshold: 250.0)\n",
            "Ep 93, Reward: -119.26, Avg (100): -93.15, Steps: 481, Total T: 73043/1000000, Time: 0:29:42\n",
            "  Avg Reward (last 20 eps): -80.86 (Threshold: 250.0)\n",
            "Ep 94, Reward: -85.19, Avg (100): -93.06, Steps: 1600, Total T: 74643/1000000, Time: 0:30:25\n",
            "  Avg Reward (last 20 eps): -81.94 (Threshold: 250.0)\n",
            "      T: 75000/1000000 | Losses C:0.13 A:-0.21 Alpha:0.00 | Alpha: 0.002\n",
            "Ep 95, Reward: -44.65, Avg (100): -92.55, Steps: 1600, Total T: 76243/1000000, Time: 0:31:10\n",
            "  Avg Reward (last 20 eps): -81.53 (Threshold: 250.0)\n",
            "Ep 96, Reward: -205.34, Avg (100): -93.73, Steps: 1536, Total T: 77779/1000000, Time: 0:31:54\n",
            "  Avg Reward (last 20 eps): -87.00 (Threshold: 250.0)\n",
            "Ep 97, Reward: -75.09, Avg (100): -93.54, Steps: 1600, Total T: 79379/1000000, Time: 0:32:38\n",
            "  Avg Reward (last 20 eps): -88.03 (Threshold: 250.0)\n",
            "      T: 80000/1000000 | Losses C:0.24 A:1.22 Alpha:-0.00 | Alpha: 0.004\n",
            "Ep 98, Reward: -95.43, Avg (100): -93.56, Steps: 1600, Total T: 80979/1000000, Time: 0:33:26\n",
            "  Avg Reward (last 20 eps): -89.71 (Threshold: 250.0)\n",
            "Ep 99, Reward: -98.45, Avg (100): -93.61, Steps: 1600, Total T: 82579/1000000, Time: 0:34:17\n",
            "  Avg Reward (last 20 eps): -92.78 (Threshold: 250.0)\n",
            "Ep 100, Reward: -73.62, Avg (100): -93.41, Steps: 1600, Total T: 84179/1000000, Time: 0:35:05\n",
            "  Avg Reward (last 20 eps): -94.90 (Threshold: 250.0)\n",
            "      T: 85000/1000000 | Losses C:0.13 A:1.31 Alpha:0.00 | Alpha: 0.002\n",
            "Ep 101, Reward: -57.52, Avg (100): -92.90, Steps: 1600, Total T: 85779/1000000, Time: 0:35:55\n",
            "  Avg Reward (last 20 eps): -95.87 (Threshold: 250.0)\n",
            "Ep 102, Reward: -50.17, Avg (100): -92.26, Steps: 1600, Total T: 87379/1000000, Time: 0:36:41\n",
            "  Avg Reward (last 20 eps): -96.41 (Threshold: 250.0)\n",
            "Ep 103, Reward: -45.43, Avg (100): -91.91, Steps: 1600, Total T: 88979/1000000, Time: 0:37:24\n",
            "  Avg Reward (last 20 eps): -96.73 (Threshold: 250.0)\n",
            "      T: 90000/1000000 | Losses C:0.12 A:1.23 Alpha:0.00 | Alpha: 0.002\n",
            "Ep 104, Reward: -48.06, Avg (100): -91.30, Steps: 1600, Total T: 90579/1000000, Time: 0:38:10\n",
            "  Avg Reward (last 20 eps): -97.55 (Threshold: 250.0)\n",
            "Ep 105, Reward: -65.60, Avg (100): -90.75, Steps: 1600, Total T: 92179/1000000, Time: 0:38:53\n",
            "  Avg Reward (last 20 eps): -95.41 (Threshold: 250.0)\n",
            "Ep 106, Reward: -43.53, Avg (100): -90.00, Steps: 1600, Total T: 93779/1000000, Time: 0:39:36\n",
            "  Avg Reward (last 20 eps): -90.48 (Threshold: 250.0)\n",
            "Ep 107, Reward: -162.50, Avg (100): -90.43, Steps: 492, Total T: 94271/1000000, Time: 0:39:49\n",
            "  Avg Reward (last 20 eps): -93.10 (Threshold: 250.0)\n",
            "Ep 108, Reward: -141.42, Avg (100): -91.03, Steps: 164, Total T: 94435/1000000, Time: 0:39:54\n",
            "  Avg Reward (last 20 eps): -94.53 (Threshold: 250.0)\n",
            "      T: 95000/1000000 | Losses C:0.43 A:1.45 Alpha:0.00 | Alpha: 0.002\n",
            "Ep 109, Reward: -78.60, Avg (100): -90.68, Steps: 1600, Total T: 96035/1000000, Time: 0:40:40\n",
            "  Avg Reward (last 20 eps): -90.64 (Threshold: 250.0)\n",
            "Ep 110, Reward: -33.01, Avg (100): -90.01, Steps: 1600, Total T: 97635/1000000, Time: 0:41:25\n",
            "  Avg Reward (last 20 eps): -86.76 (Threshold: 250.0)\n",
            "Ep 111, Reward: -20.83, Avg (100): -89.24, Steps: 1600, Total T: 99235/1000000, Time: 0:42:11\n",
            "  Avg Reward (last 20 eps): -82.21 (Threshold: 250.0)\n",
            "Ep 112, Reward: -135.88, Avg (100): -89.78, Steps: 362, Total T: 99597/1000000, Time: 0:42:22\n",
            "  Avg Reward (last 20 eps): -83.98 (Threshold: 250.0)\n",
            "      T: 100000/1000000 | Losses C:0.55 A:2.76 Alpha:-0.00 | Alpha: 0.002\n",
            "Ep 113, Reward: -7.95, Avg (100): -88.85, Steps: 1600, Total T: 101197/1000000, Time: 0:43:11\n",
            "  Avg Reward (last 20 eps): -78.41 (Threshold: 250.0)\n",
            "Ep 114, Reward: -15.00, Avg (100): -87.82, Steps: 1600, Total T: 102797/1000000, Time: 0:43:56\n",
            "  Avg Reward (last 20 eps): -74.90 (Threshold: 250.0)\n",
            "Ep 115, Reward: -11.07, Avg (100): -86.83, Steps: 1600, Total T: 104397/1000000, Time: 0:44:42\n",
            "  Avg Reward (last 20 eps): -73.23 (Threshold: 250.0)\n",
            "      T: 105000/1000000 | Losses C:0.10 A:2.55 Alpha:0.00 | Alpha: 0.002\n",
            "Ep 116, Reward: -19.74, Avg (100): -85.94, Steps: 1600, Total T: 105997/1000000, Time: 0:45:28\n",
            "  Avg Reward (last 20 eps): -63.94 (Threshold: 250.0)\n",
            "Ep 117, Reward: -13.27, Avg (100): -85.15, Steps: 1600, Total T: 107597/1000000, Time: 0:46:17\n",
            "  Avg Reward (last 20 eps): -60.85 (Threshold: 250.0)\n",
            "Ep 118, Reward: -20.96, Avg (100): -84.36, Steps: 1600, Total T: 109197/1000000, Time: 0:47:03\n",
            "  Avg Reward (last 20 eps): -57.13 (Threshold: 250.0)\n",
            "      T: 110000/1000000 | Losses C:0.04 A:1.13 Alpha:0.00 | Alpha: 0.002\n",
            "Ep 119, Reward: -19.65, Avg (100): -83.57, Steps: 1600, Total T: 110797/1000000, Time: 0:47:52\n",
            "  Avg Reward (last 20 eps): -53.19 (Threshold: 250.0)\n",
            "Ep 120, Reward: -24.37, Avg (100): -82.84, Steps: 1600, Total T: 112397/1000000, Time: 0:48:37\n",
            "  Avg Reward (last 20 eps): -50.73 (Threshold: 250.0)\n",
            "Ep 121, Reward: -17.68, Avg (100): -81.99, Steps: 1600, Total T: 113997/1000000, Time: 0:49:28\n",
            "  Avg Reward (last 20 eps): -48.74 (Threshold: 250.0)\n",
            "      T: 115000/1000000 | Losses C:0.13 A:2.04 Alpha:0.00 | Alpha: 0.002\n",
            "Ep 122, Reward: -110.06, Avg (100): -82.08, Steps: 1600, Total T: 115597/1000000, Time: 0:50:14\n",
            "  Avg Reward (last 20 eps): -51.73 (Threshold: 250.0)\n",
            "Ep 123, Reward: -21.74, Avg (100): -81.26, Steps: 1600, Total T: 117197/1000000, Time: 0:50:59\n",
            "  Avg Reward (last 20 eps): -50.55 (Threshold: 250.0)\n",
            "Ep 124, Reward: -24.74, Avg (100): -80.45, Steps: 1600, Total T: 118797/1000000, Time: 0:51:45\n",
            "  Avg Reward (last 20 eps): -49.38 (Threshold: 250.0)\n",
            "      T: 120000/1000000 | Losses C:0.37 A:2.79 Alpha:0.00 | Alpha: 0.002\n",
            "Ep 125, Reward: -8.12, Avg (100): -79.74, Steps: 1600, Total T: 120397/1000000, Time: 0:52:29\n",
            "  Avg Reward (last 20 eps): -46.51 (Threshold: 250.0)\n",
            "Ep 126, Reward: -15.62, Avg (100): -78.83, Steps: 1600, Total T: 121997/1000000, Time: 0:53:15\n",
            "  Avg Reward (last 20 eps): -45.11 (Threshold: 250.0)\n",
            "Ep 127, Reward: -14.83, Avg (100): -78.12, Steps: 1600, Total T: 123597/1000000, Time: 0:54:01\n",
            "  Avg Reward (last 20 eps): -37.73 (Threshold: 250.0)\n",
            "Ep 128, Reward: -116.72, Avg (100): -78.14, Steps: 76, Total T: 123673/1000000, Time: 0:54:03\n",
            "  Avg Reward (last 20 eps): -36.49 (Threshold: 250.0)\n",
            "Ep 129, Reward: -110.28, Avg (100): -78.09, Steps: 41, Total T: 123714/1000000, Time: 0:54:05\n",
            "  Avg Reward (last 20 eps): -38.08 (Threshold: 250.0)\n",
            "Ep 130, Reward: -123.13, Avg (100): -78.07, Steps: 132, Total T: 123846/1000000, Time: 0:54:09\n",
            "  Avg Reward (last 20 eps): -42.58 (Threshold: 250.0)\n",
            "      T: 125000/1000000 | Losses C:0.12 A:1.97 Alpha:-0.00 | Alpha: 0.002\n",
            "Ep 131, Reward: -24.41, Avg (100): -77.14, Steps: 1600, Total T: 125446/1000000, Time: 0:54:52\n",
            "  Avg Reward (last 20 eps): -42.76 (Threshold: 250.0)\n",
            "Ep 132, Reward: -109.83, Avg (100): -77.12, Steps: 48, Total T: 125494/1000000, Time: 0:54:53\n",
            "  Avg Reward (last 20 eps): -41.46 (Threshold: 250.0)\n",
            "Ep 133, Reward: -110.05, Avg (100): -77.21, Steps: 77, Total T: 125571/1000000, Time: 0:54:55\n",
            "  Avg Reward (last 20 eps): -46.56 (Threshold: 250.0)\n",
            "Ep 134, Reward: -119.40, Avg (100): -77.28, Steps: 99, Total T: 125670/1000000, Time: 0:54:58\n",
            "  Avg Reward (last 20 eps): -51.78 (Threshold: 250.0)\n",
            "Ep 135, Reward: -108.54, Avg (100): -77.43, Steps: 47, Total T: 125717/1000000, Time: 0:54:59\n",
            "  Avg Reward (last 20 eps): -56.66 (Threshold: 250.0)\n",
            "Ep 136, Reward: -36.47, Avg (100): -76.63, Steps: 1600, Total T: 127317/1000000, Time: 0:55:46\n",
            "  Avg Reward (last 20 eps): -57.49 (Threshold: 250.0)\n",
            "Ep 137, Reward: -30.62, Avg (100): -75.65, Steps: 1600, Total T: 128917/1000000, Time: 0:56:31\n",
            "  Avg Reward (last 20 eps): -58.36 (Threshold: 250.0)\n",
            "      T: 130000/1000000 | Losses C:0.18 A:1.61 Alpha:0.00 | Alpha: 0.002\n",
            "Ep 138, Reward: -25.86, Avg (100): -74.91, Steps: 1600, Total T: 130517/1000000, Time: 0:57:19\n",
            "  Avg Reward (last 20 eps): -58.61 (Threshold: 250.0)\n",
            "Ep 139, Reward: -39.15, Avg (100): -74.57, Steps: 1600, Total T: 132117/1000000, Time: 0:58:09\n",
            "  Avg Reward (last 20 eps): -59.58 (Threshold: 250.0)\n",
            "Ep 140, Reward: 26.61, Avg (100): -73.26, Steps: 1600, Total T: 133717/1000000, Time: 0:58:53\n",
            "  Avg Reward (last 20 eps): -57.03 (Threshold: 250.0)\n",
            "      T: 135000/1000000 | Losses C:0.07 A:2.05 Alpha:-0.00 | Alpha: 0.001\n",
            "Ep 141, Reward: 6.35, Avg (100): -72.15, Steps: 1600, Total T: 135317/1000000, Time: 0:59:42\n",
            "  Avg Reward (last 20 eps): -55.83 (Threshold: 250.0)\n",
            "Ep 142, Reward: -3.36, Avg (100): -71.45, Steps: 1600, Total T: 136917/1000000, Time: 1:00:29\n",
            "  Avg Reward (last 20 eps): -50.50 (Threshold: 250.0)\n",
            "Ep 143, Reward: -114.82, Avg (100): -71.80, Steps: 760, Total T: 137677/1000000, Time: 1:00:52\n",
            "  Avg Reward (last 20 eps): -55.15 (Threshold: 250.0)\n",
            "Ep 144, Reward: -18.73, Avg (100): -71.20, Steps: 1600, Total T: 139277/1000000, Time: 1:01:42\n",
            "  Avg Reward (last 20 eps): -54.85 (Threshold: 250.0)\n",
            "      T: 140000/1000000 | Losses C:0.18 A:2.44 Alpha:-0.00 | Alpha: 0.002\n",
            "Ep 145, Reward: -124.46, Avg (100): -71.65, Steps: 1424, Total T: 140701/1000000, Time: 1:02:26\n",
            "  Avg Reward (last 20 eps): -60.67 (Threshold: 250.0)\n",
            "Ep 146, Reward: 22.72, Avg (100): -70.59, Steps: 1600, Total T: 142301/1000000, Time: 1:03:12\n",
            "  Avg Reward (last 20 eps): -58.75 (Threshold: 250.0)\n",
            "Ep 147, Reward: 4.06, Avg (100): -69.76, Steps: 1600, Total T: 143901/1000000, Time: 1:03:57\n",
            "  Avg Reward (last 20 eps): -57.81 (Threshold: 250.0)\n",
            "      T: 145000/1000000 | Losses C:0.15 A:2.59 Alpha:-0.00 | Alpha: 0.002\n",
            "Ep 148, Reward: -17.07, Avg (100): -68.57, Steps: 1600, Total T: 145501/1000000, Time: 1:04:47\n",
            "  Avg Reward (last 20 eps): -52.82 (Threshold: 250.0)\n",
            "Ep 149, Reward: -11.66, Avg (100): -68.24, Steps: 1600, Total T: 147101/1000000, Time: 1:05:33\n",
            "  Avg Reward (last 20 eps): -47.89 (Threshold: 250.0)\n",
            "Ep 150, Reward: -19.08, Avg (100): -67.95, Steps: 1600, Total T: 148701/1000000, Time: 1:06:20\n",
            "  Avg Reward (last 20 eps): -42.69 (Threshold: 250.0)\n",
            "      T: 150000/1000000 | Losses C:0.06 A:2.07 Alpha:-0.00 | Alpha: 0.001\n",
            "Ep 151, Reward: -17.46, Avg (100): -67.71, Steps: 1600, Total T: 150301/1000000, Time: 1:07:07\n",
            "  Avg Reward (last 20 eps): -42.34 (Threshold: 250.0)\n",
            "Ep 152, Reward: 22.47, Avg (100): -66.39, Steps: 1600, Total T: 151901/1000000, Time: 1:07:53\n",
            "  Avg Reward (last 20 eps): -35.73 (Threshold: 250.0)\n",
            "Ep 153, Reward: 14.22, Avg (100): -65.85, Steps: 1600, Total T: 153501/1000000, Time: 1:08:43\n",
            "  Avg Reward (last 20 eps): -29.51 (Threshold: 250.0)\n",
            "      T: 155000/1000000 | Losses C:0.13 A:2.11 Alpha:-0.00 | Alpha: 0.001\n",
            "Ep 154, Reward: 10.88, Avg (100): -64.72, Steps: 1600, Total T: 155101/1000000, Time: 1:09:32\n",
            "  Avg Reward (last 20 eps): -23.00 (Threshold: 250.0)\n",
            "Ep 155, Reward: 21.52, Avg (100): -63.18, Steps: 1600, Total T: 156701/1000000, Time: 1:10:19\n",
            "  Avg Reward (last 20 eps): -16.50 (Threshold: 250.0)\n",
            "Ep 156, Reward: 19.67, Avg (100): -61.86, Steps: 1600, Total T: 158301/1000000, Time: 1:11:06\n",
            "  Avg Reward (last 20 eps): -13.69 (Threshold: 250.0)\n",
            "Ep 157, Reward: 33.40, Avg (100): -60.42, Steps: 1600, Total T: 159901/1000000, Time: 1:11:53\n",
            "  Avg Reward (last 20 eps): -10.49 (Threshold: 250.0)\n",
            "      T: 160000/1000000 | Losses C:0.04 A:1.26 Alpha:0.00 | Alpha: 0.002\n",
            "Ep 158, Reward: 35.89, Avg (100): -59.03, Steps: 1600, Total T: 161501/1000000, Time: 1:12:41\n",
            "  Avg Reward (last 20 eps): -7.40 (Threshold: 250.0)\n",
            "Ep 159, Reward: 38.33, Avg (100): -57.33, Steps: 1600, Total T: 163101/1000000, Time: 1:13:27\n",
            "  Avg Reward (last 20 eps): -3.53 (Threshold: 250.0)\n",
            "Ep 160, Reward: 37.71, Avg (100): -55.99, Steps: 1600, Total T: 164701/1000000, Time: 1:14:15\n",
            "  Avg Reward (last 20 eps): -2.97 (Threshold: 250.0)\n",
            "      T: 165000/1000000 | Losses C:0.04 A:2.03 Alpha:0.00 | Alpha: 0.002\n",
            "Ep 161, Reward: 33.74, Avg (100): -54.46, Steps: 1600, Total T: 166301/1000000, Time: 1:15:03\n",
            "  Avg Reward (last 20 eps): -1.60 (Threshold: 250.0)\n",
            "Ep 162, Reward: -100.09, Avg (100): -54.20, Steps: 971, Total T: 167272/1000000, Time: 1:15:32\n",
            "  Avg Reward (last 20 eps): -6.44 (Threshold: 250.0)\n",
            "Ep 163, Reward: 38.29, Avg (100): -53.22, Steps: 1600, Total T: 168872/1000000, Time: 1:16:22\n",
            "  Avg Reward (last 20 eps): 1.22 (Threshold: 250.0)\n",
            "      T: 170000/1000000 | Losses C:0.04 A:1.06 Alpha:-0.00 | Alpha: 0.002\n",
            "Ep 164, Reward: 24.09, Avg (100): -51.88, Steps: 1600, Total T: 170472/1000000, Time: 1:17:09\n",
            "  Avg Reward (last 20 eps): 3.36 (Threshold: 250.0)\n",
            "Ep 165, Reward: 39.19, Avg (100): -50.40, Steps: 1600, Total T: 172072/1000000, Time: 1:17:58\n",
            "  Avg Reward (last 20 eps): 11.54 (Threshold: 250.0)\n",
            "Ep 166, Reward: 31.85, Avg (100): -49.78, Steps: 1600, Total T: 173672/1000000, Time: 1:18:47\n",
            "  Avg Reward (last 20 eps): 12.00 (Threshold: 250.0)\n",
            "      T: 175000/1000000 | Losses C:0.07 A:1.72 Alpha:0.00 | Alpha: 0.002\n",
            "Ep 167, Reward: 38.17, Avg (100): -48.09, Steps: 1600, Total T: 175272/1000000, Time: 1:19:34\n",
            "  Avg Reward (last 20 eps): 13.70 (Threshold: 250.0)\n",
            "Ep 168, Reward: 35.35, Avg (100): -47.27, Steps: 1600, Total T: 176872/1000000, Time: 1:20:21\n",
            "  Avg Reward (last 20 eps): 16.32 (Threshold: 250.0)\n",
            "Ep 169, Reward: 41.29, Avg (100): -46.20, Steps: 1600, Total T: 178472/1000000, Time: 1:21:12\n",
            "  Avg Reward (last 20 eps): 18.97 (Threshold: 250.0)\n",
            "      T: 180000/1000000 | Losses C:0.14 A:1.15 Alpha:-0.00 | Alpha: 0.002\n",
            "Ep 170, Reward: 12.99, Avg (100): -45.34, Steps: 1600, Total T: 180072/1000000, Time: 1:22:01\n",
            "  Avg Reward (last 20 eps): 20.57 (Threshold: 250.0)\n",
            "Ep 171, Reward: 44.31, Avg (100): -44.11, Steps: 1600, Total T: 181672/1000000, Time: 1:22:51\n",
            "  Avg Reward (last 20 eps): 23.66 (Threshold: 250.0)\n",
            "Ep 172, Reward: 47.00, Avg (100): -42.92, Steps: 1600, Total T: 183272/1000000, Time: 1:23:41\n",
            "  Avg Reward (last 20 eps): 24.89 (Threshold: 250.0)\n",
            "Ep 173, Reward: 34.26, Avg (100): -41.89, Steps: 1600, Total T: 184872/1000000, Time: 1:24:28\n",
            "  Avg Reward (last 20 eps): 25.89 (Threshold: 250.0)\n",
            "      T: 185000/1000000 | Losses C:0.10 A:1.17 Alpha:-0.00 | Alpha: 0.002\n",
            "Ep 174, Reward: 39.51, Avg (100): -40.86, Steps: 1600, Total T: 186472/1000000, Time: 1:25:17\n",
            "  Avg Reward (last 20 eps): 27.32 (Threshold: 250.0)\n",
            "Ep 175, Reward: 40.00, Avg (100): -39.93, Steps: 1600, Total T: 188072/1000000, Time: 1:26:07\n",
            "  Avg Reward (last 20 eps): 28.25 (Threshold: 250.0)\n",
            "Ep 176, Reward: 44.77, Avg (100): -38.52, Steps: 1600, Total T: 189672/1000000, Time: 1:26:54\n",
            "  Avg Reward (last 20 eps): 29.50 (Threshold: 250.0)\n",
            "      T: 190000/1000000 | Losses C:0.04 A:0.88 Alpha:-0.00 | Alpha: 0.002\n",
            "Ep 177, Reward: 53.77, Avg (100): -37.44, Steps: 1600, Total T: 191272/1000000, Time: 1:27:42\n",
            "  Avg Reward (last 20 eps): 30.52 (Threshold: 250.0)\n",
            "Ep 178, Reward: 42.08, Avg (100): -36.40, Steps: 1600, Total T: 192872/1000000, Time: 1:28:29\n",
            "  Avg Reward (last 20 eps): 30.83 (Threshold: 250.0)\n",
            "Ep 179, Reward: 77.55, Avg (100): -35.26, Steps: 1600, Total T: 194472/1000000, Time: 1:29:15\n",
            "  Avg Reward (last 20 eps): 32.79 (Threshold: 250.0)\n",
            "      T: 195000/1000000 | Losses C:0.04 A:0.98 Alpha:-0.00 | Alpha: 0.002\n",
            "Ep 180, Reward: 67.21, Avg (100): -34.27, Steps: 1600, Total T: 196072/1000000, Time: 1:30:05\n",
            "  Avg Reward (last 20 eps): 34.27 (Threshold: 250.0)\n",
            "Ep 181, Reward: 78.74, Avg (100): -33.10, Steps: 1600, Total T: 197672/1000000, Time: 1:30:53\n",
            "  Avg Reward (last 20 eps): 36.52 (Threshold: 250.0)\n",
            "Ep 182, Reward: 85.92, Avg (100): -31.85, Steps: 1600, Total T: 199272/1000000, Time: 1:31:43\n",
            "  Avg Reward (last 20 eps): 45.82 (Threshold: 250.0)\n",
            "      T: 200000/1000000 | Losses C:0.03 A:0.52 Alpha:0.00 | Alpha: 0.002\n",
            "Ep 183, Reward: 87.90, Avg (100): -30.58, Steps: 1600, Total T: 200872/1000000, Time: 1:32:33\n",
            "  Avg Reward (last 20 eps): 48.30 (Threshold: 250.0)\n",
            "Ep 184, Reward: 87.27, Avg (100): -29.39, Steps: 1600, Total T: 202472/1000000, Time: 1:33:22\n",
            "  Avg Reward (last 20 eps): 51.46 (Threshold: 250.0)\n",
            "Ep 185, Reward: 89.08, Avg (100): -27.42, Steps: 1600, Total T: 204072/1000000, Time: 1:34:10\n",
            "  Avg Reward (last 20 eps): 53.95 (Threshold: 250.0)\n",
            "      T: 205000/1000000 | Losses C:0.03 A:0.79 Alpha:-0.00 | Alpha: 0.002\n",
            "Ep 186, Reward: 67.07, Avg (100): -25.33, Steps: 1600, Total T: 205672/1000000, Time: 1:34:59\n",
            "  Avg Reward (last 20 eps): 55.71 (Threshold: 250.0)\n",
            "Ep 187, Reward: -133.97, Avg (100): -25.57, Steps: 240, Total T: 205912/1000000, Time: 1:35:06\n",
            "  Avg Reward (last 20 eps): 47.10 (Threshold: 250.0)\n",
            "Ep 188, Reward: 69.46, Avg (100): -23.74, Steps: 1600, Total T: 207512/1000000, Time: 1:35:58\n",
            "  Avg Reward (last 20 eps): 48.81 (Threshold: 250.0)\n",
            "Ep 189, Reward: -113.69, Avg (100): -23.31, Steps: 78, Total T: 207590/1000000, Time: 1:36:00\n",
            "  Avg Reward (last 20 eps): 41.06 (Threshold: 250.0)\n",
            "Ep 190, Reward: -114.61, Avg (100): -23.35, Steps: 83, Total T: 207673/1000000, Time: 1:36:03\n",
            "  Avg Reward (last 20 eps): 34.68 (Threshold: 250.0)\n",
            "Ep 191, Reward: 108.41, Avg (100): -21.15, Steps: 1600, Total T: 209273/1000000, Time: 1:36:53\n",
            "  Avg Reward (last 20 eps): 37.89 (Threshold: 250.0)\n",
            "      T: 210000/1000000 | Losses C:0.07 A:0.21 Alpha:0.00 | Alpha: 0.002\n",
            "Ep 192, Reward: 146.55, Avg (100): -18.68, Steps: 1600, Total T: 210873/1000000, Time: 1:37:41\n",
            "  Avg Reward (last 20 eps): 42.86 (Threshold: 250.0)\n",
            "Ep 193, Reward: 142.35, Avg (100): -16.07, Steps: 1600, Total T: 212473/1000000, Time: 1:38:32\n",
            "  Avg Reward (last 20 eps): 48.27 (Threshold: 250.0)\n",
            "Ep 194, Reward: 128.29, Avg (100): -13.93, Steps: 1600, Total T: 214073/1000000, Time: 1:39:22\n",
            "  Avg Reward (last 20 eps): 52.71 (Threshold: 250.0)\n",
            "      T: 215000/1000000 | Losses C:0.10 A:-0.16 Alpha:-0.00 | Alpha: 0.003\n",
            "Ep 195, Reward: 156.68, Avg (100): -11.92, Steps: 1600, Total T: 215673/1000000, Time: 1:40:12\n",
            "  Avg Reward (last 20 eps): 58.54 (Threshold: 250.0)\n",
            "Ep 196, Reward: 144.13, Avg (100): -8.42, Steps: 1600, Total T: 217273/1000000, Time: 1:41:02\n",
            "  Avg Reward (last 20 eps): 63.51 (Threshold: 250.0)\n",
            "Ep 197, Reward: -24.86, Avg (100): -7.92, Steps: 1158, Total T: 218431/1000000, Time: 1:41:39\n",
            "  Avg Reward (last 20 eps): 59.58 (Threshold: 250.0)\n",
            "      T: 220000/1000000 | Losses C:0.09 A:-0.31 Alpha:0.00 | Alpha: 0.003\n",
            "Ep 198, Reward: 123.71, Avg (100): -5.73, Steps: 1600, Total T: 220031/1000000, Time: 1:42:28\n",
            "  Avg Reward (last 20 eps): 63.66 (Threshold: 250.0)\n",
            "Ep 199, Reward: -62.88, Avg (100): -5.37, Steps: 550, Total T: 220581/1000000, Time: 1:42:45\n",
            "  Avg Reward (last 20 eps): 56.64 (Threshold: 250.0)\n",
            "Ep 200, Reward: -78.26, Avg (100): -5.42, Steps: 535, Total T: 221116/1000000, Time: 1:43:02\n",
            "  Avg Reward (last 20 eps): 49.36 (Threshold: 250.0)\n",
            "Ep 201, Reward: 152.70, Avg (100): -3.32, Steps: 1600, Total T: 222716/1000000, Time: 1:43:53\n",
            "  Avg Reward (last 20 eps): 53.06 (Threshold: 250.0)\n",
            "Ep 202, Reward: 131.37, Avg (100): -1.50, Steps: 1600, Total T: 224316/1000000, Time: 1:44:42\n",
            "  Avg Reward (last 20 eps): 55.33 (Threshold: 250.0)\n",
            "      T: 225000/1000000 | Losses C:0.15 A:-1.07 Alpha:0.00 | Alpha: 0.004\n",
            "Ep 203, Reward: 147.67, Avg (100): 0.43, Steps: 1600, Total T: 225916/1000000, Time: 1:45:35\n",
            "  Avg Reward (last 20 eps): 58.32 (Threshold: 250.0)\n",
            "Ep 204, Reward: 135.59, Avg (100): 2.26, Steps: 1600, Total T: 227516/1000000, Time: 1:46:28\n",
            "  Avg Reward (last 20 eps): 60.74 (Threshold: 250.0)\n",
            "Ep 205, Reward: 159.74, Avg (100): 4.52, Steps: 1600, Total T: 229116/1000000, Time: 1:47:19\n",
            "  Avg Reward (last 20 eps): 64.27 (Threshold: 250.0)\n",
            "      T: 230000/1000000 | Losses C:0.07 A:-1.29 Alpha:0.00 | Alpha: 0.003\n",
            "Ep 206, Reward: 170.79, Avg (100): 6.66, Steps: 1600, Total T: 230716/1000000, Time: 1:48:10\n",
            "  Avg Reward (last 20 eps): 69.46 (Threshold: 250.0)\n",
            "Ep 207, Reward: 153.51, Avg (100): 9.82, Steps: 1600, Total T: 232316/1000000, Time: 1:49:02\n",
            "  Avg Reward (last 20 eps): 83.83 (Threshold: 250.0)\n",
            "Ep 208, Reward: 140.16, Avg (100): 12.64, Steps: 1600, Total T: 233916/1000000, Time: 1:49:55\n",
            "  Avg Reward (last 20 eps): 87.37 (Threshold: 250.0)\n",
            "      T: 235000/1000000 | Losses C:0.07 A:-1.07 Alpha:-0.00 | Alpha: 0.003\n",
            "Ep 209, Reward: 147.57, Avg (100): 14.90, Steps: 1600, Total T: 235516/1000000, Time: 1:50:49\n",
            "  Avg Reward (last 20 eps): 100.43 (Threshold: 250.0)\n",
            "Ep 210, Reward: 151.82, Avg (100): 16.75, Steps: 1600, Total T: 237116/1000000, Time: 1:51:41\n",
            "  Avg Reward (last 20 eps): 113.75 (Threshold: 250.0)\n",
            "Ep 211, Reward: 177.86, Avg (100): 18.73, Steps: 1600, Total T: 238716/1000000, Time: 1:52:34\n",
            "  Avg Reward (last 20 eps): 117.22 (Threshold: 250.0)\n",
            "      T: 240000/1000000 | Losses C:0.10 A:-1.85 Alpha:0.00 | Alpha: 0.003\n",
            "Ep 212, Reward: 172.08, Avg (100): 21.81, Steps: 1600, Total T: 240316/1000000, Time: 1:53:27\n",
            "  Avg Reward (last 20 eps): 118.50 (Threshold: 250.0)\n",
            "Ep 213, Reward: 193.60, Avg (100): 23.83, Steps: 1600, Total T: 241916/1000000, Time: 1:54:20\n",
            "  Avg Reward (last 20 eps): 121.06 (Threshold: 250.0)\n",
            "Ep 214, Reward: 199.40, Avg (100): 25.97, Steps: 1600, Total T: 243516/1000000, Time: 1:55:13\n",
            "  Avg Reward (last 20 eps): 124.62 (Threshold: 250.0)\n",
            "      T: 245000/1000000 | Losses C:0.05 A:-2.25 Alpha:0.00 | Alpha: 0.004\n",
            "Ep 215, Reward: 201.67, Avg (100): 28.10, Steps: 1600, Total T: 245116/1000000, Time: 1:56:07\n",
            "  Avg Reward (last 20 eps): 126.87 (Threshold: 250.0)\n",
            "Ep 216, Reward: 199.78, Avg (100): 30.30, Steps: 1600, Total T: 246716/1000000, Time: 1:57:02\n",
            "  Avg Reward (last 20 eps): 129.65 (Threshold: 250.0)\n",
            "Ep 217, Reward: 228.40, Avg (100): 32.71, Steps: 1600, Total T: 248316/1000000, Time: 1:57:56\n",
            "  Avg Reward (last 20 eps): 142.31 (Threshold: 250.0)\n",
            "Ep 218, Reward: 199.94, Avg (100): 34.92, Steps: 1600, Total T: 249916/1000000, Time: 1:58:49\n",
            "  Avg Reward (last 20 eps): 146.12 (Threshold: 250.0)\n",
            "      T: 250000/1000000 | Losses C:0.04 A:-2.34 Alpha:-0.00 | Alpha: 0.004\n",
            "Ep 219, Reward: 226.03, Avg (100): 37.38, Steps: 1600, Total T: 251516/1000000, Time: 1:59:42\n",
            "  Avg Reward (last 20 eps): 160.57 (Threshold: 250.0)\n",
            "Ep 220, Reward: 243.91, Avg (100): 40.06, Steps: 1600, Total T: 253116/1000000, Time: 2:00:35\n",
            "  Avg Reward (last 20 eps): 176.68 (Threshold: 250.0)\n",
            "Ep 221, Reward: 259.87, Avg (100): 42.84, Steps: 1600, Total T: 254716/1000000, Time: 2:01:26\n",
            "  Avg Reward (last 20 eps): 182.04 (Threshold: 250.0)\n",
            "      T: 255000/1000000 | Losses C:0.07 A:-3.35 Alpha:0.00 | Alpha: 0.004\n",
            "Ep 222, Reward: 233.46, Avg (100): 46.27, Steps: 1600, Total T: 256316/1000000, Time: 2:02:20\n",
            "  Avg Reward (last 20 eps): 187.14 (Threshold: 250.0)\n",
            "Ep 223, Reward: 259.07, Avg (100): 49.08, Steps: 1600, Total T: 257916/1000000, Time: 2:03:15\n",
            "  Avg Reward (last 20 eps): 192.71 (Threshold: 250.0)\n",
            "Ep 224, Reward: 223.47, Avg (100): 51.56, Steps: 1600, Total T: 259516/1000000, Time: 2:04:10\n",
            "  Avg Reward (last 20 eps): 197.11 (Threshold: 250.0)\n",
            "      T: 260000/1000000 | Losses C:0.14 A:-4.10 Alpha:0.00 | Alpha: 0.004\n",
            "Ep 225, Reward: 245.50, Avg (100): 54.10, Steps: 1600, Total T: 261116/1000000, Time: 2:05:09\n",
            "  Avg Reward (last 20 eps): 201.39 (Threshold: 250.0)\n",
            "Ep 226, Reward: 266.22, Avg (100): 56.92, Steps: 1600, Total T: 262716/1000000, Time: 2:06:05\n",
            "  Avg Reward (last 20 eps): 206.17 (Threshold: 250.0)\n",
            "Ep 227, Reward: 14.27, Avg (100): 57.21, Steps: 964, Total T: 263680/1000000, Time: 2:06:40\n",
            "  Avg Reward (last 20 eps): 199.20 (Threshold: 250.0)\n",
            "      T: 265000/1000000 | Losses C:0.10 A:-3.51 Alpha:-0.00 | Alpha: 0.005\n",
            "Ep 228, Reward: 247.38, Avg (100): 60.85, Steps: 1600, Total T: 265280/1000000, Time: 2:07:36\n",
            "  Avg Reward (last 20 eps): 204.56 (Threshold: 250.0)\n",
            "Ep 229, Reward: 230.80, Avg (100): 64.26, Steps: 1600, Total T: 266880/1000000, Time: 2:08:30\n",
            "  Avg Reward (last 20 eps): 208.73 (Threshold: 250.0)\n",
            "Ep 230, Reward: 281.25, Avg (100): 68.30, Steps: 1600, Total T: 268480/1000000, Time: 2:09:26\n",
            "  Avg Reward (last 20 eps): 215.20 (Threshold: 250.0)\n",
            "      T: 270000/1000000 | Losses C:0.07 A:-3.86 Alpha:-0.00 | Alpha: 0.005\n",
            "Ep 231, Reward: 274.52, Avg (100): 71.29, Steps: 1600, Total T: 270080/1000000, Time: 2:10:24\n",
            "  Avg Reward (last 20 eps): 220.03 (Threshold: 250.0)\n",
            "Ep 232, Reward: 93.46, Avg (100): 73.33, Steps: 1237, Total T: 271317/1000000, Time: 2:11:10\n",
            "  Avg Reward (last 20 eps): 216.10 (Threshold: 250.0)\n",
            "Ep 233, Reward: 285.67, Avg (100): 77.28, Steps: 1569, Total T: 272886/1000000, Time: 2:12:09\n",
            "  Avg Reward (last 20 eps): 220.70 (Threshold: 250.0)\n",
            "Ep 234, Reward: 284.32, Avg (100): 81.32, Steps: 1515, Total T: 274401/1000000, Time: 2:13:16\n",
            "  Avg Reward (last 20 eps): 224.95 (Threshold: 250.0)\n",
            "      T: 275000/1000000 | Losses C:0.08 A:-4.38 Alpha:-0.00 | Alpha: 0.005\n",
            "Ep 235, Reward: 290.72, Avg (100): 85.31, Steps: 1448, Total T: 275849/1000000, Time: 2:14:10\n",
            "  Avg Reward (last 20 eps): 229.40 (Threshold: 250.0)\n",
            "Ep 236, Reward: -2.34, Avg (100): 85.65, Steps: 580, Total T: 276429/1000000, Time: 2:14:31\n",
            "  Avg Reward (last 20 eps): 219.30 (Threshold: 250.0)\n",
            "Ep 237, Reward: 289.64, Avg (100): 88.86, Steps: 1418, Total T: 277847/1000000, Time: 2:15:25\n",
            "  Avg Reward (last 20 eps): 222.36 (Threshold: 250.0)\n",
            "Ep 238, Reward: 292.69, Avg (100): 92.04, Steps: 1377, Total T: 279224/1000000, Time: 2:16:16\n",
            "  Avg Reward (last 20 eps): 226.99 (Threshold: 250.0)\n",
            "      T: 280000/1000000 | Losses C:0.16 A:-4.91 Alpha:0.00 | Alpha: 0.005\n",
            "Ep 239, Reward: 286.96, Avg (100): 95.30, Steps: 1489, Total T: 280713/1000000, Time: 2:17:10\n",
            "  Avg Reward (last 20 eps): 230.04 (Threshold: 250.0)\n",
            "Ep 240, Reward: 286.29, Avg (100): 97.90, Steps: 1494, Total T: 282207/1000000, Time: 2:18:04\n",
            "  Avg Reward (last 20 eps): 232.16 (Threshold: 250.0)\n",
            "Ep 241, Reward: 288.29, Avg (100): 100.72, Steps: 1457, Total T: 283664/1000000, Time: 2:18:57\n",
            "  Avg Reward (last 20 eps): 233.58 (Threshold: 250.0)\n",
            "      T: 285000/1000000 | Losses C:0.06 A:-5.35 Alpha:-0.00 | Alpha: 0.006\n",
            "Ep 242, Reward: 292.56, Avg (100): 103.68, Steps: 1364, Total T: 285028/1000000, Time: 2:19:48\n",
            "  Avg Reward (last 20 eps): 236.54 (Threshold: 250.0)\n",
            "Ep 243, Reward: 283.37, Avg (100): 107.66, Steps: 1500, Total T: 286528/1000000, Time: 2:20:42\n",
            "  Avg Reward (last 20 eps): 237.75 (Threshold: 250.0)\n",
            "Ep 244, Reward: 289.01, Avg (100): 110.74, Steps: 1441, Total T: 287969/1000000, Time: 2:21:31\n",
            "  Avg Reward (last 20 eps): 241.03 (Threshold: 250.0)\n",
            "Ep 245, Reward: 292.33, Avg (100): 114.91, Steps: 1355, Total T: 289324/1000000, Time: 2:22:19\n",
            "  Avg Reward (last 20 eps): 243.37 (Threshold: 250.0)\n",
            "      T: 290000/1000000 | Losses C:0.06 A:-5.80 Alpha:0.00 | Alpha: 0.006\n",
            "Ep 246, Reward: 288.52, Avg (100): 117.56, Steps: 1328, Total T: 290652/1000000, Time: 2:23:05\n",
            "  Avg Reward (last 20 eps): 244.49 (Threshold: 250.0)\n",
            "Ep 247, Reward: 170.06, Avg (100): 119.22, Steps: 1298, Total T: 291950/1000000, Time: 2:23:52\n",
            "  Avg Reward (last 20 eps): 252.27 (Threshold: 250.0)\n",
            "\n",
            "--- Early Stopping Condition Met! ---\n",
            "Average reward over last 20 episodes (252.27) exceeded threshold (250.0).\n",
            "\n",
            "--- Training Finished (Early Stop) ---\n",
            "Total Timesteps: 291950\n",
            "Total Episodes: 247\n",
            "Total Time: 2:23:52\n",
            "\n",
            "--- Starting Final Evaluation (10 episodes) ---\n",
            "  Eval Episode 1/10 | Reward: 295.18\n",
            "  Eval Episode 2/10 | Reward: 287.40\n",
            "  Eval Episode 3/10 | Reward: 294.13\n",
            "  Eval Episode 4/10 | Reward: 292.31\n",
            "  Eval Episode 5/10 | Reward: 294.21\n",
            "  Eval Episode 6/10 | Reward: 292.65\n",
            "  Eval Episode 7/10 | Reward: 290.48\n",
            "  Eval Episode 8/10 | Reward: 293.93\n",
            "  Eval Episode 9/10 | Reward: 51.54\n",
            "  Eval Episode 10/10 | Reward: 293.51\n",
            "--- Final Evaluation Finished | Average Reward: 268.54 ---\n",
            "--- Final Model Saved ---\n",
            "Actor: sac_actor_BipedalWalker-v3_earlystop_291950.pth\n",
            "Critic: sac_critic_BipedalWalker-v3_earlystop_291950.pth\n",
            "Script finished.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "name": "ashku458 (May 4, 2025, 6:56:03PM)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}